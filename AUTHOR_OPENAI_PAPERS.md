| date_published 	| author 	| blog_name 	| blog_link 	| paper_link 	|
|---	|---	|---	|---	|---	|
| October 18, 2016 	| martin-abadi 	| Semi-supervised knowledge transfer for deep learning from private training data 	| https://openai.com/research/semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data 	| https://arxiv.org/abs/1610.05755 	|
| July 26, 2018 	| pieter-abbeel 	| Variational option discovery algorithms 	| https://openai.com/research/variational-option-discovery-algorithms 	| https://arxiv.org/abs/1807.10299 	|
| April 18, 2018 	| pieter-abbeel 	| Evolved Policy Gradients 	| https://openai.com/research/evolved-policy-gradients 	| https://arxiv.org/abs/1802.04821 	|
| March 20, 2018 	| pieter-abbeel 	| Variance reduction for policy gradient with action-dependent factorized baselines 	| https://openai.com/research/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines 	| https://arxiv.org/abs/1803.07246 	|
| March 3, 2018 	| pieter-abbeel 	| Some considerations on learning to explore via meta-reinforcement learning 	| https://openai.com/research/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning 	| https://arxiv.org/abs/1803.01118 	|
| February 15, 2018 	| pieter-abbeel 	| Interpretable machine learning through teaching 	| https://openai.com/research/interpretable-machine-learning-through-teaching 	| https://arxiv.org/abs/1711.00694 	|
| November 2, 2017 	| pieter-abbeel 	| Interpretable and pedagogical examples 	| https://openai.com/research/interpretable-and-pedagogical-examples 	| https://arxiv.org/abs/1711.00694 	|
| October 26, 2017 	| pieter-abbeel 	| Learning a hierarchy 	| https://openai.com/research/learning-a-hierarchy 	| https://arxiv.org/abs/1710.09767 	|
| October 19, 2017 	| pieter-abbeel 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 18, 2017 	| pieter-abbeel 	| Asymmetric actor critic for image-based robot learning 	| https://openai.com/research/asymmetric-actor-critic-for-image-based-robot-learning 	|  	|
| October 18, 2017 	| pieter-abbeel 	| Sim-to-real transfer of robotic control with dynamics randomization 	| https://openai.com/research/sim-to-real-transfer-of-robotic-control-with-dynamics-randomization 	| https://arxiv.org/abs/1710.06537 	|
| October 17, 2017 	| pieter-abbeel 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| October 11, 2017 	| pieter-abbeel 	| Meta-learning for wrestling 	| https://openai.com/research/meta-learning-for-wrestling 	| https://arxiv.org/abs/1710.03641 	|
| September 14, 2017 	| pieter-abbeel 	| Learning to model other minds 	| https://openai.com/research/learning-to-model-other-minds 	| https://arxiv.org/abs/1709.04326 	|
| September 13, 2017 	| pieter-abbeel 	| Learning with opponent-learning awareness 	| https://openai.com/research/learning-with-opponent-learning-awareness 	| https://arxiv.org/abs/1709.04326 	|
| July 27, 2017 	| pieter-abbeel 	| Better exploration with parameter noise 	| https://openai.com/research/better-exploration-with-parameter-noise 	| https://arxiv.org/abs/1706.01905 	|
| July 5, 2017 	| pieter-abbeel 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| June 8, 2017 	| pieter-abbeel 	| Learning to cooperate, compete, and communicate 	| https://openai.com/research/learning-to-cooperate-compete-and-communicate 	| https://arxiv.org/abs/1706.02275 	|
| June 5, 2017 	| pieter-abbeel 	| UCB exploration via Q-ensembles 	| https://openai.com/research/ucb-exploration-via-q-ensembles 	| https://arxiv.org/abs/1706.01502 	|
| May 16, 2017 	| pieter-abbeel 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 21, 2017 	| pieter-abbeel 	| Equivalence between policy gradients and soft Q-learning 	| https://openai.com/research/equivalence-between-policy-gradients-and-soft-q-learning 	| https://arxiv.org/abs/1704.06440 	|
| July 25, 2022 	| joshua-achiam 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| July 7, 2021 	| joshua-achiam 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| November 21, 2019 	| joshua-achiam 	| Benchmarking safe exploration in deep reinforcement learning 	| https://openai.com/research/benchmarking-safe-exploration-in-deep-reinforcement-learning 	| https://cdn.openai.com/safexp-short.pdf 	|
| November 21, 2019 	| joshua-achiam 	| Safety Gym 	| https://openai.com/research/safety-gym 	| https://cdn.openai.com/safexp-short.pdf 	|
| November 8, 2018 	| joshua-achiam 	| Spinning Up in Deep RL 	| https://openai.com/research/spinning-up-in-deep-rl 	|  	|
| July 26, 2018 	| joshua-achiam 	| Variational option discovery algorithms 	| https://openai.com/research/variational-option-discovery-algorithms 	| https://arxiv.org/abs/1807.10299 	|
| March 8, 2018 	| joshua-achiam 	| On first-order meta-learning algorithms 	| https://openai.com/research/on-first-order-meta-learning-algorithms 	| https://arxiv.org/abs/1803.02999 	|
| March 3, 2022 	| steven-adler 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| March 3, 2022 	| sandhini-agarwal 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| January 5, 2021 	| sandhini-agarwal 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| January 5, 2021 	| sandhini-agarwal 	| CLIP: Connecting text and images 	| https://openai.com/research/clip 	| https://arxiv.org/abs/2103.00020 	|
| May 28, 2020 	| sandhini-agarwal 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| June 23, 2022 	| ilge-akkaya 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| October 15, 2019 	| ilge-akkaya 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| June 17, 2018 	| maruan-al-shedivat 	| Learning policy representations in multiagent systems 	| https://openai.com/research/learning-policy-representations-in-multiagent-systems 	| https://arxiv.org/abs/1806.06464 	|
| October 11, 2017 	| maruan-al-shedivat 	| Meta-learning for wrestling 	| https://openai.com/research/meta-learning-for-wrestling 	| https://arxiv.org/abs/1710.03641 	|
| September 14, 2017 	| maruan-al-shedivat 	| Learning to model other minds 	| https://openai.com/research/learning-to-model-other-minds 	| https://arxiv.org/abs/1709.04326 	|
| September 13, 2017 	| maruan-al-shedivat 	| Learning with opponent-learning awareness 	| https://openai.com/research/learning-with-opponent-learning-awareness 	| https://arxiv.org/abs/1709.04326 	|
| February 14, 2019 	| daniella-amodei 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| March 4, 2021 	| daniela-amodei 	| Multimodal neurons in artificial neural networks 	| https://openai.com/research/multimodal-neurons 	| https://distill.pub/2021/multimodal-neurons/ 	|
| October 15, 2019 	| marcin-andrychowicz 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 30, 2018 	| marcin-andrychowicz 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| marcin-andrychowicz 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| marcin-andrychowicz 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| October 19, 2017 	| marcin-andrychowicz 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 18, 2017 	| marcin-andrychowicz 	| Asymmetric actor critic for image-based robot learning 	| https://openai.com/research/asymmetric-actor-critic-for-image-based-robot-learning 	|  	|
| October 18, 2017 	| marcin-andrychowicz 	| Sim-to-real transfer of robotic control with dynamics randomization 	| https://openai.com/research/sim-to-real-transfer-of-robotic-control-with-dynamics-randomization 	| https://arxiv.org/abs/1710.06537 	|
| October 17, 2017 	| marcin-andrychowicz 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| July 27, 2017 	| marcin-andrychowicz 	| Better exploration with parameter noise 	| https://openai.com/research/better-exploration-with-parameter-noise 	| https://arxiv.org/abs/1706.01905 	|
| July 5, 2017 	| marcin-andrychowicz 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| May 16, 2017 	| marcin-andrychowicz 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| March 21, 2017 	| marcin-andrychowicz 	| One-shot imitation learning 	| https://openai.com/research/one-shot-imitation-learning 	| https://arxiv.org/abs/1703.07326 	|
| March 17, 2023 	| tasmin-asfour 	| GPTs are GPTs: An early look at the labor market impact potential of large language models 	| https://openai.com/research/gpts-are-gpts 	| https://arxiv.org/abs/2303.10130 	|
| March 14, 2023 	| tasmin-asfour 	| GPT-4 	| https://openai.com/research/gpt-4 	| https://arxiv.org/abs/2303.08774 	|
| January 11, 2023 	| tasmin-asfour 	| Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk 	| https://openai.com/research/forecasting-misuse 	| https://arxiv.org/abs/2301.04246 	|
| December 16, 2022 	| tasmin-asfour 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| October 19, 2022 	| tasmin-asfour 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| September 21, 2022 	| tasmin-asfour 	| Introducing Whisper 	| https://openai.com/research/whisper 	| https://cdn.openai.com/papers/whisper.pdf 	|
| July 28, 2022 	| tasmin-asfour 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 25, 2022 	| tasmin-asfour 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| June 28, 2022 	| tasmin-asfour 	| DALL·E 2 pre-training mitigations 	| https://openai.com/research/dall-e-2-pre-training-mitigations 	|  	|
| June 23, 2022 	| tasmin-asfour 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| June 17, 2022 	| tasmin-asfour 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| June 13, 2022 	| tasmin-asfour 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| June 9, 2022 	| tasmin-asfour 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| May 28, 2022 	| tasmin-asfour 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| April 13, 2022 	| tasmin-asfour 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| April 13, 2022 	| tasmin-asfour 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| March 3, 2022 	| tasmin-asfour 	| A research agenda for assessing the economic impacts of code generation models 	| https://openai.com/research/economic-impacts 	| https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf 	|
| March 3, 2022 	| tasmin-asfour 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| February 2, 2022 	| tasmin-asfour 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| January 27, 2022 	| tasmin-asfour 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| May 28, 2020 	| amanda-askell 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| July 10, 2019 	| amanda-askell 	| Why responsible AI development needs cooperation on safety 	| https://openai.com/research/cooperation-on-safety 	| http://arxiv.org/abs/1907.04534 	|
| February 19, 2019 	| amanda-askell 	| AI safety needs social scientists 	| https://openai.com/research/ai-safety-needs-social-scientists 	| https://distill.pub/2019/safety-needs-social-scientists 	|
| February 14, 2019 	| amanda-askell 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| July 17, 2017 	| anish-athalye 	| Robust adversarial inputs 	| https://openai.com/research/robust-adversarial-inputs 	| https://arxiv.org/abs/1707.07397 	|
| July 7, 2021 	| igor-babuschkin 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| June 23, 2022 	| bowen-baker 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| September 17, 2019 	| bowen-baker 	| Emergent tool use from multi-agent interaction 	| https://openai.com/research/emergent-tool-use 	| https://arxiv.org/abs/1909.07528 	|
| July 30, 2018 	| bowen-baker 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| bowen-baker 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| bowen-baker 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| December 5, 2019 	| yamini-bansal 	| Deep double descent 	| https://openai.com/research/deep-double-descent 	| https://arxiv.org/abs/1912.02292 	|
| December 5, 2019 	| boaz-barak 	| Deep double descent 	| https://openai.com/research/deep-double-descent 	| https://arxiv.org/abs/1912.02292 	|
| March 17, 2023 	| elizabeth-barnes 	| GPTs are GPTs: An early look at the labor market impact potential of large language models 	| https://openai.com/research/gpts-are-gpts 	| https://arxiv.org/abs/2303.10130 	|
| March 14, 2023 	| elizabeth-barnes 	| GPT-4 	| https://openai.com/research/gpt-4 	| https://arxiv.org/abs/2303.08774 	|
| January 11, 2023 	| elizabeth-barnes 	| Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk 	| https://openai.com/research/forecasting-misuse 	| https://arxiv.org/abs/2301.04246 	|
| December 16, 2022 	| elizabeth-barnes 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| October 19, 2022 	| elizabeth-barnes 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| September 21, 2022 	| elizabeth-barnes 	| Introducing Whisper 	| https://openai.com/research/whisper 	| https://cdn.openai.com/papers/whisper.pdf 	|
| July 28, 2022 	| elizabeth-barnes 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 25, 2022 	| elizabeth-barnes 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| June 28, 2022 	| elizabeth-barnes 	| DALL·E 2 pre-training mitigations 	| https://openai.com/research/dall-e-2-pre-training-mitigations 	|  	|
| June 23, 2022 	| elizabeth-barnes 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| June 17, 2022 	| elizabeth-barnes 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| June 13, 2022 	| elizabeth-barnes 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| June 9, 2022 	| elizabeth-barnes 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| May 28, 2022 	| elizabeth-barnes 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| April 13, 2022 	| elizabeth-barnes 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| April 13, 2022 	| elizabeth-barnes 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| March 3, 2022 	| elizabeth-barnes 	| A research agenda for assessing the economic impacts of code generation models 	| https://openai.com/research/economic-impacts 	| https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf 	|
| March 3, 2022 	| elizabeth-barnes 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| February 2, 2022 	| elizabeth-barnes 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| January 27, 2022 	| elizabeth-barnes 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| July 30, 2018 	| ben-barry 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| November 9, 2016 	| peter-l-bartlett 	| RL²: Fast reinforcement learning via slow reinforcement learning 	| https://openai.com/research/rl2 	| https://arxiv.org/abs/1611.02779 	|
| July 28, 2022 	| mohammad-bavarian 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 7, 2021 	| mohammad-bavarian 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| March 20, 2018 	| alexandre-m-bayen 	| Variance reduction for policy gradient with action-dependent factorized baselines 	| https://openai.com/research/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines 	| https://arxiv.org/abs/1803.07246 	|
| May 28, 2020 	| christopher-berner 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| December 13, 2019 	| christopher-berner 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| christopher-berner 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| January 18, 2018 	| christopher-berner 	| Scaling Kubernetes to 2,500 nodes 	| https://openai.com/research/scaling-kubernetes-to-2500-nodes 	|  	|
| October 2, 2018 	| jesse-bettencourt 	| FFJORD: Free-form continuous dynamics for scalable reversible generative models 	| https://openai.com/research/ffjord 	| https://arxiv.org/abs/1810.01367 	|
| October 17, 2017 	| lukas-biewald 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| October 19, 2017 	| xue-bin-peng 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 18, 2017 	| xue-bin-peng 	| Sim-to-real transfer of robotic control with dynamics randomization 	| https://openai.com/research/sim-to-real-transfer-of-robotic-control-with-dynamics-randomization 	| https://arxiv.org/abs/1710.06537 	|
| October 11, 2016 	| trevor-blackwell 	| Transfer from simulation to real world through learning deep inverse dynamics model 	| https://openai.com/research/transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model 	| https://arxiv.org/abs/1610.03518 	|
| June 9, 2022 	| greg-brockman 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| July 7, 2021 	| greg-brockman 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| December 13, 2019 	| greg-brockman 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| July 30, 2018 	| greg-brockman 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| June 25, 2018 	| greg-brockman 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| May 16, 2018 	| greg-brockman 	| AI and compute 	| https://openai.com/research/ai-and-compute 	|  	|
| May 16, 2017 	| greg-brockman 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 6, 2017 	| greg-brockman 	| Unsupervised sentiment neuron 	| https://openai.com/research/unsupervised-sentiment-neuron 	| https://arxiv.org/abs/1704.01444 	|
| March 24, 2017 	| greg-brockman 	| Evolution strategies as a scalable alternative to reinforcement learning 	| https://openai.com/research/evolution-strategies 	| https://arxiv.org/abs/1703.03864 	|
| August 29, 2016 	| greg-brockman 	| Infrastructure for deep learning 	| https://openai.com/research/infrastructure-for-deep-learning 	|  	|
| June 21, 2016 	| greg-brockman 	| Concrete AI safety problems 	| https://openai.com/research/concrete-ai-safety-problems 	| https://arxiv.org/abs/1606.06565 	|
| June 16, 2016 	| greg-brockman 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| April 27, 2016 	| greg-brockman 	| OpenAI Gym Beta 	| https://openai.com/research/openai-gym-beta 	| https://arxiv.org/abs/1606.01540 	|
| May 28, 2020 	| tom-brown 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| May 5, 2020 	| tom-brown 	| AI and efficiency 	| https://openai.com/research/ai-and-efficiency 	| https://arxiv.org/abs/2005.04305 	|
| January 23, 2020 	| tom-brown 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| September 19, 2019 	| tom-brown 	| Fine-tuning GPT-2 from human preferences 	| https://openai.com/research/fine-tuning-gpt-2 	| https://arxiv.org/abs/1909.08593 	|
| May 3, 2019 	| tom-brown 	| Transfer of adversarial robustness between perturbation types 	| https://openai.com/research/transfer-of-adversarial-robustness-between-perturbation-types 	| https://arxiv.org/abs/1905.01034 	|
| August 3, 2017 	| tom-brown 	| Gathering human feedback 	| https://openai.com/research/gathering-human-feedback 	|  	|
| July 25, 2022 	| miles-brundage 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| March 3, 2022 	| miles-brundage 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| July 7, 2021 	| miles-brundage 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| February 4, 2021 	| miles-brundage 	| Understanding the capabilities, limitations, and societal impact of large language models 	| https://openai.com/research/understanding-the-capabilities-limitations-and-societal-impact-of-large-language-models 	| https://arxiv.org/abs/2102.02503 	|
| July 10, 2019 	| miles-brundage 	| Why responsible AI development needs cooperation on safety 	| https://openai.com/research/cooperation-on-safety 	| http://arxiv.org/abs/1907.04534 	|
| February 14, 2019 	| miles-brundage 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| July 7, 2021 	| yura-burda 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 31, 2018 	| yura-burda 	| Reinforcement learning with prediction-based rewards 	| https://openai.com/research/reinforcement-learning-with-prediction-based-rewards 	| https://arxiv.org/abs/1810.12894 	|
| August 13, 2018 	| yura-burda 	| Large-scale study of curiosity-driven learning 	| https://openai.com/research/large-scale-study-of-curiosity-driven-learning 	| https://arxiv.org/abs/1808.04355 	|
| June 17, 2018 	| yura-burda 	| Learning policy representations in multiagent systems 	| https://openai.com/research/learning-policy-representations-in-multiagent-systems 	| https://arxiv.org/abs/1806.06464 	|
| October 11, 2017 	| yura-burda 	| Meta-learning for wrestling 	| https://openai.com/research/meta-learning-for-wrestling 	| https://arxiv.org/abs/1710.03641 	|
| November 14, 2016 	| yura-burda 	| On the quantitative analysis of decoder-based generative models 	| https://openai.com/research/on-the-quantitative-analysis-of-decoder-based-generative-models 	| https://arxiv.org/abs/1611.04273 	|
| March 4, 2021 	| nick-cammarata 	| Multimodal neurons in artificial neural networks 	| https://openai.com/research/multimodal-neurons 	| https://distill.pub/2021/multimodal-neurons/ 	|
| April 14, 2020 	| nick-cammarata 	| OpenAI Microscope 	| https://openai.com/research/microscope 	|  	|
| July 7, 2021 	| andrew-n-carr 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| March 4, 2021 	| shan-carter 	| Multimodal neurons in artificial neural networks 	| https://openai.com/research/multimodal-neurons 	| https://distill.pub/2021/multimodal-neurons/ 	|
| April 14, 2020 	| shan-carter 	| OpenAI Microscope 	| https://openai.com/research/microscope 	|  	|
| July 7, 2021 	| brooke-chan 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| December 13, 2019 	| brooke-chan 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| brooke-chan 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| July 7, 2021 	| fotios-chantzis 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 26, 2017 	| peter-chen 	| Learning a hierarchy 	| https://openai.com/research/learning-a-hierarchy 	| https://arxiv.org/abs/1710.09767 	|
| March 24, 2017 	| peter-chen 	| Evolution strategies as a scalable alternative to reinforcement learning 	| https://openai.com/research/evolution-strategies 	| https://arxiv.org/abs/1703.03864 	|
| June 16, 2016 	| peter-chen 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| July 4, 2018 	| richard-chen 	| Learning Montezuma’s Revenge from a single demonstration 	| https://openai.com/research/learning-montezumas-revenge-from-a-single-demonstration 	|  	|
| April 18, 2018 	| richard-chen 	| Evolved Policy Gradients 	| https://openai.com/research/evolved-policy-gradients 	| https://arxiv.org/abs/1802.04821 	|
| September 14, 2017 	| richard-chen 	| Learning to model other minds 	| https://openai.com/research/learning-to-model-other-minds 	| https://arxiv.org/abs/1709.04326 	|
| September 13, 2017 	| richard-chen 	| Learning with opponent-learning awareness 	| https://openai.com/research/learning-with-opponent-learning-awareness 	| https://arxiv.org/abs/1709.04326 	|
| July 27, 2017 	| richard-chen 	| Better exploration with parameter noise 	| https://openai.com/research/better-exploration-with-parameter-noise 	| https://arxiv.org/abs/1706.01905 	|
| June 5, 2017 	| richard-chen 	| UCB exploration via Q-ensembles 	| https://openai.com/research/ucb-exploration-via-q-ensembles 	| https://arxiv.org/abs/1706.01502 	|
| March 3, 2018 	| xi-chen 	| Some considerations on learning to explore via meta-reinforcement learning 	| https://openai.com/research/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning 	| https://arxiv.org/abs/1803.01118 	|
| July 27, 2017 	| xi-chen 	| Better exploration with parameter noise 	| https://openai.com/research/better-exploration-with-parameter-noise 	| https://arxiv.org/abs/1706.01905 	|
| April 21, 2017 	| xi-chen 	| Equivalence between policy gradients and soft Q-learning 	| https://openai.com/research/equivalence-between-policy-gradients-and-soft-q-learning 	| https://arxiv.org/abs/1704.06440 	|
| January 19, 2017 	| xi-chen 	| PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications 	| https://openai.com/research/pixelcnn-plus-plus 	| https://arxiv.org/abs/1701.05517 	|
| November 15, 2016 	| xi-chen 	| #Exploration: A study of count-based exploration for deep reinforcement learning 	| https://openai.com/research/exploration 	| https://arxiv.org/abs/1611.04717 	|
| November 9, 2016 	| xi-chen 	| RL²: Fast reinforcement learning via slow reinforcement learning 	| https://openai.com/research/rl2 	| https://arxiv.org/abs/1611.02779 	|
| November 8, 2016 	| xi-chen 	| Variational lossy autoencoder 	| https://openai.com/research/variational-lossy-autoencoder 	| https://arxiv.org/abs/1611.02731 	|
| December 16, 2022 	| mark-chen 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| July 28, 2022 	| mark-chen 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| April 13, 2022 	| mark-chen 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| July 7, 2021 	| mark-chen 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 5, 2021 	| mark-chen 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| June 17, 2020 	| mark-chen 	| Image GPT 	| https://openai.com/research/image-gpt 	| https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf 	|
| May 28, 2020 	| mark-chen 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| October 2, 2018 	| ricky-t-q-chen 	| FFJORD: Free-form continuous dynamics for scalable reversible generative models 	| https://openai.com/research/ffjord 	| https://arxiv.org/abs/1810.01367 	|
| January 25, 2021 	| benjamin-chess 	| Scaling Kubernetes to 7,500 nodes 	| https://openai.com/research/scaling-kubernetes-to-7500-nodes 	|  	|
| May 28, 2020 	| benjamin-chess 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| January 23, 2020 	| benjamin-chess 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| December 13, 2019 	| vicki-cheung 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| August 29, 2016 	| vicki-cheung 	| Infrastructure for deep learning 	| https://openai.com/research/infrastructure-for-deep-learning 	|  	|
| June 16, 2016 	| vicki-cheung 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| January 5, 2021 	| rewon-child 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| May 28, 2020 	| rewon-child 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| January 23, 2020 	| rewon-child 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| April 23, 2019 	| rewon-child 	| Generative modeling with sparse transformers 	| https://openai.com/research/sparse-transformer 	| https://arxiv.org/abs/1904.10509 	|
| October 15, 2019 	| maciek-chociej 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 30, 2018 	| maciek-chociej 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| maciek-chociej 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| maciek-chociej 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| September 4, 2020 	| paul-christiano 	| Learning to summarize with human feedback 	| https://openai.com/research/learning-to-summarize-with-human-feedback 	| https://arxiv.org/abs/2009.01325 	|
| September 19, 2019 	| paul-christiano 	| Fine-tuning GPT-2 from human preferences 	| https://openai.com/research/fine-tuning-gpt-2 	| https://arxiv.org/abs/1909.08593 	|
| October 22, 2018 	| paul-christiano 	| Learning complex goals with iterated amplification 	| https://openai.com/research/learning-complex-goals-with-iterated-amplification 	| https://arxiv.org/abs/1810.08575 	|
| June 25, 2018 	| paul-christiano 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| August 3, 2017 	| paul-christiano 	| Gathering human feedback 	| https://openai.com/research/gathering-human-feedback 	|  	|
| June 13, 2017 	| paul-christiano 	| Learning from human preferences 	| https://openai.com/research/learning-from-human-preferences 	| https://arxiv.org/abs/1706.03741 	|
| November 11, 2016 	| paul-christiano 	| A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models 	| https://openai.com/research/a-connection-between-generative-adversarial-networks-inverse-reinforcement-learning-and-energy-based-models 	| https://arxiv.org/abs/1611.03852 	|
| October 11, 2016 	| paul-christiano 	| Transfer from simulation to real world through learning deep inverse dynamics model 	| https://openai.com/research/transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model 	| https://arxiv.org/abs/1610.03518 	|
| June 21, 2016 	| paul-christiano 	| Concrete AI safety problems 	| https://openai.com/research/concrete-ai-safety-problems 	| https://arxiv.org/abs/1606.06565 	|
| April 13, 2022 	| casey-chu 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| February 4, 2021 	| jack-clark 	| Understanding the capabilities, limitations, and societal impact of large language models 	| https://openai.com/research/understanding-the-capabilities-limitations-and-societal-impact-of-large-language-models 	| https://arxiv.org/abs/2102.02503 	|
| May 28, 2020 	| jack-clark 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| July 10, 2019 	| jack-clark 	| Why responsible AI development needs cooperation on safety 	| https://openai.com/research/cooperation-on-safety 	| http://arxiv.org/abs/1907.04534 	|
| February 14, 2019 	| jack-clark 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| July 30, 2018 	| jack-clark 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| June 25, 2018 	| jack-clark 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| May 16, 2018 	| jack-clark 	| AI and compute 	| https://openai.com/research/ai-and-compute 	|  	|
| February 20, 2018 	| jack-clark 	| Preparing for malicious uses of AI 	| https://openai.com/research/preparing-for-malicious-uses-of-ai 	| https://arxiv.org/abs/1802.07228 	|
| May 16, 2017 	| jack-clark 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 6, 2017 	| jack-clark 	| Unsupervised sentiment neuron 	| https://openai.com/research/unsupervised-sentiment-neuron 	| https://arxiv.org/abs/1704.01444 	|
| April 1, 2017 	| jack-clark 	| Spam detection in the physical world 	| https://openai.com/research/spam-detection-in-the-physical-world 	|  	|
| March 16, 2017 	| jack-clark 	| Learning to communicate 	| https://openai.com/research/learning-to-communicate 	|  	|
| February 24, 2017 	| jack-clark 	| Attacking machine learning with adversarial examples 	| https://openai.com/research/attacking-machine-learning-with-adversarial-examples 	|  	|
| December 21, 2016 	| jack-clark 	| Faulty reward functions in the wild 	| https://openai.com/research/faulty-reward-functions 	|  	|
| June 23, 2022 	| jeff-clune 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| October 29, 2021 	| karl-cobbe 	| Solving math word problems 	| https://openai.com/research/solving-math-word-problems 	| http://arxiv.org/abs/2110.14168 	|
| December 3, 2019 	| karl-cobbe 	| Procgen Benchmark 	| https://openai.com/research/procgen-benchmark 	| https://arxiv.org/abs/1912.01588 	|
| December 6, 2018 	| karl-cobbe 	| Quantifying generalization in reinforcement learning 	| https://openai.com/research/quantifying-generalization-in-reinforcement-learning 	| https://arxiv.org/abs/1812.02341 	|
| July 1, 2017 	| taco-cohen 	| Teacher–student curriculum learning 	| https://openai.com/research/teacher-student-curriculum-learning 	| https://arxiv.org/abs/1707.00183 	|
| July 7, 2021 	| dave-cummings 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 25, 2016 	| andrew-m-dai 	| Adversarial training methods for semi-supervised text classification 	| https://openai.com/research/adversarial-training-methods-for-semi-supervised-text-classification 	| https://arxiv.org/abs/1605.07725 	|
| August 13, 2018 	| trevor-darrell 	| Large-scale study of curiosity-driven learning 	| https://openai.com/research/large-scale-study-of-curiosity-driven-learning 	| https://arxiv.org/abs/1808.04355 	|
| December 13, 2019 	| przemyslaw-debiak 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| przemyslaw-debiak 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| February 4, 2019 	| akshay-degwekar 	| Computational limitations in robust classification and win-win results 	| https://openai.com/research/computational-limitations-in-robust-classification-and-win-win-results 	| https://arxiv.org/abs/1902.01086 	|
| June 10, 2021 	| christy-dennison 	| Improving language model behavior by training on a curated dataset 	| https://openai.com/research/improving-language-model-behavior 	| https://cdn.openai.com/palms.pdf 	|
| December 13, 2019 	| christy-dennison 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| christy-dennison 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| November 15, 2016 	| filip-de-turck 	| #Exploration: A study of count-based exploration for deep reinforcement learning 	| https://openai.com/research/exploration 	| https://arxiv.org/abs/1611.04717 	|
| December 16, 2022 	| prafulla-dhariwal 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| April 13, 2022 	| prafulla-dhariwal 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| May 28, 2020 	| prafulla-dhariwal 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| April 30, 2020 	| prafulla-dhariwal 	| Jukebox 	| https://openai.com/research/jukebox 	| https://arxiv.org/abs/2005.00341 	|
| July 9, 2018 	| prafulla-dhariwal 	| Glow: Better reversible generative models 	| https://openai.com/research/glow 	| https://arxiv.org/abs/1807.03039 	|
| June 2, 2018 	| prafulla-dhariwal 	| GamePad: A learning environment for theorem proving 	| https://openai.com/research/gamepad 	| https://arxiv.org/abs/1806.00608 	|
| July 27, 2017 	| prafulla-dhariwal 	| Better exploration with parameter noise 	| https://openai.com/research/better-exploration-with-parameter-noise 	| https://arxiv.org/abs/1706.01905 	|
| July 20, 2017 	| prafulla-dhariwal 	| Proximal Policy Optimization 	| https://openai.com/research/openai-baselines-ppo 	| https://arxiv.org/abs/1707.06347 	|
| November 8, 2016 	| prafulla-dhariwal 	| Variational lossy autoencoder 	| https://openai.com/research/variational-lossy-autoencoder 	| https://arxiv.org/abs/1611.02731 	|
| March 21, 2019 	| yilun-du 	| Implicit generation and generalization methods for energy-based models 	| https://openai.com/research/energy-based-models 	| http://arxiv.org/abs/1903.08689 	|
| March 4, 2019 	| yilun-du 	| Neural MMO: A massively multiagent game environment 	| https://openai.com/research/neural-mmo 	| http://arxiv.org/abs/1903.00784 	|
| March 20, 2018 	| yan-duan 	| Variance reduction for policy gradient with action-dependent factorized baselines 	| https://openai.com/research/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines 	| https://arxiv.org/abs/1803.07246 	|
| March 3, 2018 	| yan-duan 	| Some considerations on learning to explore via meta-reinforcement learning 	| https://openai.com/research/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning 	| https://arxiv.org/abs/1803.01118 	|
| October 17, 2017 	| yan-duan 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| May 16, 2017 	| yan-duan 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 10, 2017 	| yan-duan 	| Stochastic Neural Networks for hierarchical reinforcement learning 	| https://openai.com/research/stochastic-neural-networks-for-hierarchical-reinforcement-learning 	| https://arxiv.org/abs/1704.03012 	|
| March 21, 2017 	| yan-duan 	| One-shot imitation learning 	| https://openai.com/research/one-shot-imitation-learning 	| https://arxiv.org/abs/1703.07326 	|
| February 24, 2017 	| yan-duan 	| Attacking machine learning with adversarial examples 	| https://openai.com/research/attacking-machine-learning-with-adversarial-examples 	|  	|
| February 8, 2017 	| yan-duan 	| Adversarial attacks on neural network policies 	| https://openai.com/research/adversarial-attacks-on-neural-network-policies 	| https://arxiv.org/abs/1702.02284 	|
| November 15, 2016 	| yan-duan 	| #Exploration: A study of count-based exploration for deep reinforcement learning 	| https://openai.com/research/exploration 	| https://arxiv.org/abs/1611.04717 	|
| November 9, 2016 	| yan-duan 	| RL²: Fast reinforcement learning via slow reinforcement learning 	| https://openai.com/research/rl2 	| https://arxiv.org/abs/1611.02779 	|
| November 8, 2016 	| yan-duan 	| Variational lossy autoencoder 	| https://openai.com/research/variational-lossy-autoencoder 	| https://arxiv.org/abs/1611.02731 	|
| June 16, 2016 	| yan-duan 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| March 17, 2023 	| david-duvenaud 	| GPTs are GPTs: An early look at the labor market impact potential of large language models 	| https://openai.com/research/gpts-are-gpts 	| https://arxiv.org/abs/2303.10130 	|
| March 14, 2023 	| david-duvenaud 	| GPT-4 	| https://openai.com/research/gpt-4 	| https://arxiv.org/abs/2303.08774 	|
| January 11, 2023 	| david-duvenaud 	| Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk 	| https://openai.com/research/forecasting-misuse 	| https://arxiv.org/abs/2301.04246 	|
| December 16, 2022 	| david-duvenaud 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| October 19, 2022 	| david-duvenaud 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| September 21, 2022 	| david-duvenaud 	| Introducing Whisper 	| https://openai.com/research/whisper 	| https://cdn.openai.com/papers/whisper.pdf 	|
| July 28, 2022 	| david-duvenaud 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 25, 2022 	| david-duvenaud 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| June 28, 2022 	| david-duvenaud 	| DALL·E 2 pre-training mitigations 	| https://openai.com/research/dall-e-2-pre-training-mitigations 	|  	|
| June 23, 2022 	| david-duvenaud 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| June 17, 2022 	| david-duvenaud 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| June 13, 2022 	| david-duvenaud 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| June 9, 2022 	| david-duvenaud 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| May 28, 2022 	| david-duvenaud 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| April 13, 2022 	| david-duvenaud 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| April 13, 2022 	| david-duvenaud 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| March 3, 2022 	| david-duvenaud 	| A research agenda for assessing the economic impacts of code generation models 	| https://openai.com/research/economic-impacts 	| https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf 	|
| March 3, 2022 	| david-duvenaud 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| February 2, 2022 	| david-duvenaud 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| January 27, 2022 	| david-duvenaud 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| July 7, 2021 	| harri-edwards 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 31, 2018 	| harri-edwards 	| Reinforcement learning with prediction-based rewards 	| https://openai.com/research/reinforcement-learning-with-prediction-based-rewards 	| https://arxiv.org/abs/1810.12894 	|
| August 13, 2018 	| harri-edwards 	| Large-scale study of curiosity-driven learning 	| https://openai.com/research/large-scale-study-of-curiosity-driven-learning 	| https://arxiv.org/abs/1808.04355 	|
| July 26, 2018 	| harri-edwards 	| Variational option discovery algorithms 	| https://openai.com/research/variational-option-discovery-algorithms 	| https://arxiv.org/abs/1807.10299 	|
| June 17, 2018 	| harri-edwards 	| Learning policy representations in multiagent systems 	| https://openai.com/research/learning-policy-representations-in-multiagent-systems 	| https://arxiv.org/abs/1806.06464 	|
| August 13, 2018 	| alexei-a-efros 	| Large-scale study of curiosity-driven learning 	| https://openai.com/research/large-scale-study-of-curiosity-driven-learning 	| https://arxiv.org/abs/1808.04355 	|
| March 3, 2022 	| tyna-eloundou 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| January 24, 2022 	| tyna-eloundou 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| October 18, 2016 	| ulfar-erlingsson 	| Semi-supervised knowledge transfer for deep learning from private training data 	| https://openai.com/research/semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data 	| https://arxiv.org/abs/1610.05755 	|
| May 28, 2022 	| owain-evans 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| September 8, 2021 	| owain-evans 	| TruthfulQA: Measuring how models mimic human falsehoods 	| https://openai.com/research/truthfulqa 	| https://arxiv.org/abs/2109.07958 	|
| December 13, 2019 	| david-farhi 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| david-farhi 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| November 11, 2016 	| chelsea-finn 	| A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models 	| https://openai.com/research/a-connection-between-generative-adversarial-networks-inverse-reinforcement-learning-and-energy-based-models 	| https://arxiv.org/abs/1611.03852 	|
| December 13, 2019 	| quirin-fischer 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| quirin-fischer 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| April 10, 2017 	| carlos-florensa 	| Stochastic Neural Networks for hierarchical reinforcement learning 	| https://openai.com/research/stochastic-neural-networks-for-hierarchical-reinforcement-learning 	| https://arxiv.org/abs/1704.03012 	|
| September 29, 2017 	| jakob-foerster 	| Nonlinear computation in deep linear networks 	| https://openai.com/research/nonlinear-computation-in-deep-linear-networks 	|  	|
| September 14, 2017 	| jakob-foerster 	| Learning to model other minds 	| https://openai.com/research/learning-to-model-other-minds 	| https://arxiv.org/abs/1709.04326 	|
| September 13, 2017 	| jakob-foerster 	| Learning with opponent-learning awareness 	| https://openai.com/research/learning-with-opponent-learning-awareness 	| https://arxiv.org/abs/1709.04326 	|
| July 5, 2017 	| rachel-fong 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| May 16, 2017 	| rachel-fong 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 1, 2017 	| rachel-fong 	| Spam detection in the physical world 	| https://openai.com/research/spam-detection-in-the-physical-world 	|  	|
| November 15, 2016 	| davis-foote 	| #Exploration: A study of count-based exploration for deep reinforcement learning 	| https://openai.com/research/exploration 	| https://arxiv.org/abs/1611.04717 	|
| October 26, 2017 	| kevin-frans 	| Learning a hierarchy 	| https://openai.com/research/learning-a-hierarchy 	| https://arxiv.org/abs/1710.09767 	|
| February 4, 2021 	| deep-ganguli 	| Understanding the capabilities, limitations, and societal impact of large language models 	| https://openai.com/research/understanding-the-capabilities-limitations-and-societal-impact-of-large-language-models 	| https://arxiv.org/abs/2102.02503 	|
| October 19, 2022 	| leo-gao 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| April 13, 2022 	| leo-gao 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| March 16, 2017 	| jon-gauthier 	| Learning to communicate 	| https://openai.com/research/learning-to-communicate 	|  	|
| March 4, 2021 	| gabriel-goh 	| Multimodal neurons in artificial neural networks 	| https://openai.com/research/multimodal-neurons 	| https://distill.pub/2021/multimodal-neurons/ 	|
| January 5, 2021 	| gabriel-goh 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| April 14, 2020 	| gabriel-goh 	| OpenAI Microscope 	| https://openai.com/research/microscope 	|  	|
| February 24, 2017 	| ian-goodfellow 	| Attacking machine learning with adversarial examples 	| https://openai.com/research/attacking-machine-learning-with-adversarial-examples 	|  	|
| February 8, 2017 	| ian-goodfellow 	| Adversarial attacks on neural network policies 	| https://openai.com/research/adversarial-attacks-on-neural-network-policies 	| https://arxiv.org/abs/1702.02284 	|
| October 18, 2016 	| ian-goodfellow 	| Semi-supervised knowledge transfer for deep learning from private training data 	| https://openai.com/research/semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data 	| https://arxiv.org/abs/1610.05755 	|
| June 16, 2016 	| ian-goodfellow 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| May 25, 2016 	| ian-goodfellow 	| Adversarial training methods for semi-supervised text classification 	| https://openai.com/research/adversarial-training-methods-for-semi-supervised-text-classification 	| https://arxiv.org/abs/1605.07725 	|
| June 17, 2022 	| jonathan-gordon 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| October 2, 2018 	| will-grathwohl 	| FFJORD: Free-form continuous dynamics for scalable reversible generative models 	| https://openai.com/research/ffjord 	| https://arxiv.org/abs/1810.01367 	|
| July 7, 2021 	| scott-gray 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 5, 2021 	| scott-gray 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| May 28, 2020 	| scott-gray 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| January 23, 2020 	| scott-gray 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| December 13, 2019 	| scott-gray 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| April 23, 2019 	| scott-gray 	| Generative modeling with sparse transformers 	| https://openai.com/research/sparse-transformer 	| https://arxiv.org/abs/1904.10509 	|
| June 25, 2018 	| scott-gray 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| December 6, 2017 	| scott-gray 	| Block-sparse GPU kernels 	| https://openai.com/research/block-sparse-gpu-kernels 	| https://cdn.openai.com/blocksparse/blocksparsepaper.pdf 	|
| November 14, 2016 	| roger-grosse 	| On the quantitative analysis of decoder-based generative models 	| https://openai.com/research/on-the-quantitative-analysis-of-decoder-based-generative-models 	| https://arxiv.org/abs/1611.04273 	|
| March 17, 2023 	| aditya-grover 	| GPTs are GPTs: An early look at the labor market impact potential of large language models 	| https://openai.com/research/gpts-are-gpts 	| https://arxiv.org/abs/2303.10130 	|
| March 14, 2023 	| aditya-grover 	| GPT-4 	| https://openai.com/research/gpt-4 	| https://arxiv.org/abs/2303.08774 	|
| January 11, 2023 	| aditya-grover 	| Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk 	| https://openai.com/research/forecasting-misuse 	| https://arxiv.org/abs/2301.04246 	|
| December 16, 2022 	| aditya-grover 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| October 19, 2022 	| aditya-grover 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| September 21, 2022 	| aditya-grover 	| Introducing Whisper 	| https://openai.com/research/whisper 	| https://cdn.openai.com/papers/whisper.pdf 	|
| July 28, 2022 	| aditya-grover 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 25, 2022 	| aditya-grover 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| June 28, 2022 	| aditya-grover 	| DALL·E 2 pre-training mitigations 	| https://openai.com/research/dall-e-2-pre-training-mitigations 	|  	|
| June 23, 2022 	| aditya-grover 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| June 17, 2022 	| aditya-grover 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| June 13, 2022 	| aditya-grover 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| June 9, 2022 	| aditya-grover 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| May 28, 2022 	| aditya-grover 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| April 13, 2022 	| aditya-grover 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| April 13, 2022 	| aditya-grover 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| March 3, 2022 	| aditya-grover 	| A research agenda for assessing the economic impacts of code generation models 	| https://openai.com/research/economic-impacts 	| https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf 	|
| March 3, 2022 	| aditya-grover 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| February 2, 2022 	| aditya-grover 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| January 27, 2022 	| aditya-grover 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| June 17, 2018 	| jayesh-k-gupta 	| Learning policy representations in multiagent systems 	| https://openai.com/research/learning-policy-representations-in-multiagent-systems 	| https://arxiv.org/abs/1806.06464 	|
| July 7, 2021 	| william-guss 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 24, 2022 	| chris-hallacy 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| February 2, 2022 	| jesse-michael-han 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| January 24, 2022 	| jesse-michael-han 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| October 17, 2017 	| ankur-handa 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| May 16, 2017 	| ankur-handa 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| June 8, 2017 	| jean-harb 	| Learning to cooperate, compete, and communicate 	| https://openai.com/research/learning-to-cooperate-compete-and-communicate 	| https://arxiv.org/abs/1706.02275 	|
| December 13, 2019 	| shariq-hashme 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| shariq-hashme 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| January 24, 2022 	| johannes-heidecke 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| May 3, 2019 	| dan-hendrycks 	| Transfer of adversarial robustness between perturbation types 	| https://openai.com/research/transfer-of-adversarial-robustness-between-perturbation-types 	| https://arxiv.org/abs/1905.01034 	|
| May 28, 2020 	| tom-henighan 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| January 23, 2020 	| tom-henighan 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| July 7, 2021 	| ariel-herbert-voss 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 28, 2020 	| ariel-herbert-voss 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| May 5, 2020 	| danny-hernandez 	| AI and efficiency 	| https://openai.com/research/ai-and-efficiency 	| https://arxiv.org/abs/2005.04305 	|
| February 14, 2019 	| danny-hernandez 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| May 16, 2018 	| danny-hernandez 	| AI and compute 	| https://openai.com/research/ai-and-compute 	|  	|
| July 7, 2021 	| christopher-hesse 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 28, 2020 	| christopher-hesse 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| December 13, 2019 	| christopher-hesse 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| December 3, 2019 	| christopher-hesse 	| Procgen Benchmark 	| https://openai.com/research/procgen-benchmark 	| https://arxiv.org/abs/1912.01588 	|
| June 25, 2018 	| christopher-hesse 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| June 22, 2018 	| christopher-hesse 	| Retro Contest: Results 	| https://openai.com/research/retro-contest-results 	|  	|
| May 25, 2018 	| christopher-hesse 	| Gym Retro 	| https://openai.com/research/gym-retro 	|  	|
| April 10, 2018 	| christopher-hesse 	| Gotta Learn Fast: A new benchmark for generalization in RL 	| https://openai.com/research/gotta-learn-fast 	| https://arxiv.org/abs/1804.03720 	|
| April 5, 2018 	| christopher-hesse 	| Retro Contest 	| https://openai.com/research/retro-contest 	| https://arxiv.org/abs/1804.03720 	|
| October 19, 2022 	| jacob-hilton 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| May 28, 2022 	| jacob-hilton 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| April 13, 2022 	| jacob-hilton 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| December 16, 2021 	| jacob-hilton 	| WebGPT: Improving the factual accuracy of language models through web browsing 	| https://openai.com/research/webgpt 	| https://arxiv.org/abs/2112.09332 	|
| September 8, 2021 	| jacob-hilton 	| TruthfulQA: Measuring how models mimic human falsehoods 	| https://openai.com/research/truthfulqa 	| https://arxiv.org/abs/2109.07958 	|
| December 3, 2019 	| jacob-hilton 	| Procgen Benchmark 	| https://openai.com/research/procgen-benchmark 	| https://arxiv.org/abs/1912.01588 	|
| April 18, 2018 	| jonathan-ho 	| Evolved Policy Gradients 	| https://openai.com/research/evolved-policy-gradients 	| https://arxiv.org/abs/1802.04821 	|
| October 26, 2017 	| jonathan-ho 	| Learning a hierarchy 	| https://openai.com/research/learning-a-hierarchy 	| https://arxiv.org/abs/1710.09767 	|
| June 28, 2017 	| jonathan-ho 	| Faster physics in Python 	| https://openai.com/research/faster-physics-in-python 	|  	|
| May 16, 2017 	| jonathan-ho 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| March 24, 2017 	| jonathan-ho 	| Evolution strategies as a scalable alternative to reinforcement learning 	| https://openai.com/research/evolution-strategies 	| https://arxiv.org/abs/1703.03864 	|
| March 21, 2017 	| jonathan-ho 	| One-shot imitation learning 	| https://openai.com/research/one-shot-imitation-learning 	| https://arxiv.org/abs/1703.07326 	|
| June 16, 2016 	| jonathan-ho 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| April 18, 2018 	| rein-houthooft 	| Evolved Policy Gradients 	| https://openai.com/research/evolved-policy-gradients 	| https://arxiv.org/abs/1802.04821 	|
| March 3, 2018 	| rein-houthooft 	| Some considerations on learning to explore via meta-reinforcement learning 	| https://openai.com/research/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning 	| https://arxiv.org/abs/1803.01118 	|
| July 27, 2017 	| rein-houthooft 	| Better exploration with parameter noise 	| https://openai.com/research/better-exploration-with-parameter-noise 	| https://arxiv.org/abs/1706.01905 	|
| November 15, 2016 	| rein-houthooft 	| #Exploration: A study of count-based exploration for deep reinforcement learning 	| https://openai.com/research/exploration 	| https://arxiv.org/abs/1611.04717 	|
| June 16, 2016 	| rein-houthooft 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| January 24, 2022 	| kenny-hsu 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| February 24, 2017 	| sandy-huang 	| Attacking machine learning with adversarial examples 	| https://openai.com/research/attacking-machine-learning-with-adversarial-examples 	|  	|
| February 8, 2017 	| sandy-huang 	| Adversarial attacks on neural network policies 	| https://openai.com/research/adversarial-attacks-on-neural-network-policies 	| https://arxiv.org/abs/1702.02284 	|
| June 2, 2018 	| daniel-huang 	| GamePad: A learning environment for theorem proving 	| https://openai.com/research/gamepad 	| https://arxiv.org/abs/1806.00608 	|
| June 23, 2022 	| joost-huizinga 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| September 19, 2019 	| geoffrey-irving 	| Fine-tuning GPT-2 from human preferences 	| https://openai.com/research/fine-tuning-gpt-2 	| https://arxiv.org/abs/1909.08593 	|
| February 19, 2019 	| geoffrey-irving 	| AI safety needs social scientists 	| https://openai.com/research/ai-safety-needs-social-scientists 	| https://distill.pub/2019/safety-needs-social-scientists 	|
| May 3, 2018 	| geoffrey-irving 	| AI safety via debate 	| https://openai.com/research/debate 	| https://arxiv.org/abs/1805.00899 	|
| March 4, 2019 	| phillip-isola 	| Neural MMO: A massively multiagent game environment 	| https://openai.com/research/neural-mmo 	| http://arxiv.org/abs/1903.00784 	|
| April 18, 2018 	| phillip-isola 	| Evolved Policy Gradients 	| https://openai.com/research/evolved-policy-gradients 	| https://arxiv.org/abs/1802.04821 	|
| July 7, 2021 	| shantanu-jain 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| June 17, 2022 	| shawn-jain 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| January 24, 2022 	| joanne-jang 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| nicholas-joseph 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| December 13, 2019 	| rafal-jozefowicz 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| July 30, 2018 	| rafal-jozefowicz 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| April 6, 2017 	| rafal-jozefowicz 	| Unsupervised sentiment neuron 	| https://openai.com/research/unsupervised-sentiment-neuron 	| https://arxiv.org/abs/1704.01444 	|
| December 16, 2022 	| heewoo-jun 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| July 28, 2022 	| heewoo-jun 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 7, 2021 	| heewoo-jun 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| April 30, 2020 	| heewoo-jun 	| Jukebox 	| https://openai.com/research/jukebox 	| https://arxiv.org/abs/2005.00341 	|
| July 7, 2021 	| lukasz-kaiser 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| November 5, 2018 	| sham-kakade 	| Plan online, learn offline: Efficient learning and exploration via model-based control 	| https://openai.com/research/plan-online-learn-offline 	| https://arxiv.org/abs/1811.01848 	|
| March 20, 2018 	| sham-kakade 	| Variance reduction for policy gradient with action-dependent factorized baselines 	| https://openai.com/research/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines 	| https://arxiv.org/abs/1803.07246 	|
| May 3, 2019 	| daniel-kang 	| Transfer of adversarial robustness between perturbation types 	| https://openai.com/research/transfer-of-adversarial-robustness-between-perturbation-types 	| https://arxiv.org/abs/1905.01034 	|
| September 17, 2019 	| ingmar-kanitscheider 	| Emergent tool use from multi-agent interaction 	| https://openai.com/research/emergent-tool-use 	| https://arxiv.org/abs/1909.07528 	|
| July 7, 2021 	| jared-kaplan 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 28, 2020 	| jared-kaplan 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| January 23, 2020 	| jared-kaplan 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| December 14, 2018 	| jared-kaplan 	| How AI training scales 	| https://openai.com/research/how-ai-training-scales 	| https://arxiv.org/pdf/1812.06162.pdf 	|
| December 5, 2019 	| gal-kaplun 	| Deep double descent 	| https://openai.com/research/deep-double-descent 	| https://arxiv.org/abs/1912.02292 	|
| March 24, 2017 	| andrej-karpathy 	| Evolution strategies as a scalable alternative to reinforcement learning 	| https://openai.com/research/evolution-strategies 	| https://arxiv.org/abs/1703.03864 	|
| January 19, 2017 	| andrej-karpathy 	| PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications 	| https://openai.com/research/pixelcnn-plus-plus 	| https://arxiv.org/abs/1701.05517 	|
| June 16, 2016 	| andrej-karpathy 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| January 24, 2022 	| tabarak-khan 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 25, 2022 	| heidy-khlaaf 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| July 7, 2021 	| heidy-khlaaf 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 24, 2022 	| jong-wook-kim 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| January 5, 2021 	| jong-wook-kim 	| CLIP: Connecting text and images 	| https://openai.com/research/clip 	| https://arxiv.org/abs/2103.00020 	|
| April 30, 2020 	| jong-wook-kim 	| Jukebox 	| https://openai.com/research/jukebox 	| https://arxiv.org/abs/2005.00341 	|
| July 9, 2018 	| durk-kingma 	| Glow: Better reversible generative models 	| https://openai.com/research/glow 	| https://arxiv.org/abs/1807.03039 	|
| January 31, 2018 	| durk-kingma 	| Requests for Research 2.0 	| https://openai.com/research/requests-for-research-2 	|  	|
| December 6, 2017 	| durk-kingma 	| Block-sparse GPU kernels 	| https://openai.com/research/block-sparse-gpu-kernels 	| https://cdn.openai.com/blocksparse/blocksparsepaper.pdf 	|
| December 4, 2017 	| durk-kingma 	| Learning sparse neural networks through L₀ regularization 	| https://openai.com/research/learning-sparse-neural-networks-through-l0-regularization 	| https://arxiv.org/abs/1712.01312 	|
| January 19, 2017 	| durk-kingma 	| PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications 	| https://openai.com/research/pixelcnn-plus-plus 	| https://arxiv.org/abs/1701.05517 	|
| November 8, 2016 	| durk-kingma 	| Variational lossy autoencoder 	| https://openai.com/research/variational-lossy-autoencoder 	| https://arxiv.org/abs/1611.02731 	|
| June 16, 2016 	| durk-kingma 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| February 25, 2016 	| durk-kingma 	| Weight normalization: A simple reparameterization to accelerate training of deep neural networks 	| https://openai.com/research/weight-normalization 	| https://arxiv.org/abs/1602.07868 	|
| June 22, 2018 	| oleg-klimov 	| Retro Contest: Results 	| https://openai.com/research/retro-contest-results 	|  	|
| May 25, 2018 	| oleg-klimov 	| Gym Retro 	| https://openai.com/research/gym-retro 	|  	|
| April 10, 2018 	| oleg-klimov 	| Gotta Learn Fast: A new benchmark for generalization in RL 	| https://openai.com/research/gotta-learn-fast 	| https://arxiv.org/abs/1804.03720 	|
| April 5, 2018 	| oleg-klimov 	| Retro Contest 	| https://openai.com/research/retro-contest 	| https://arxiv.org/abs/1804.03720 	|
| July 20, 2017 	| oleg-klimov 	| Proximal Policy Optimization 	| https://openai.com/research/openai-baselines-ppo 	| https://arxiv.org/abs/1707.06347 	|
| July 7, 2021 	| matthew-knight 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 29, 2021 	| vineet-kosaraju 	| Solving math word problems 	| https://openai.com/research/solving-math-word-problems 	| http://arxiv.org/abs/2110.14168 	|
| July 25, 2022 	| gretchen-krueger 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| March 3, 2022 	| gretchen-krueger 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| January 24, 2022 	| gretchen-krueger 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| gretchen-krueger 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 5, 2021 	| gretchen-krueger 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| January 5, 2021 	| gretchen-krueger 	| CLIP: Connecting text and images 	| https://openai.com/research/clip 	| https://arxiv.org/abs/2103.00020 	|
| May 28, 2020 	| gretchen-krueger 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| March 20, 2018 	| vikash-kumar 	| Variance reduction for policy gradient with action-dependent factorized baselines 	| https://openai.com/research/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines 	| https://arxiv.org/abs/1803.07246 	|
| February 26, 2018 	| vikash-kumar 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| vikash-kumar 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| October 17, 2017 	| vikash-kumar 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| May 16, 2017 	| vikash-kumar 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| February 14, 2019 	| david-lansky 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| June 17, 2022 	| joel-lehman 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| June 13, 2022 	| jan-leike 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| March 3, 2022 	| jan-leike 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| January 27, 2022 	| jan-leike 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| September 23, 2021 	| jan-leike 	| Summarizing books with human feedback 	| https://openai.com/research/summarizing-books 	|  	|
| July 7, 2021 	| jan-leike 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| November 11, 2016 	| sergey-levine 	| A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models 	| https://openai.com/research/a-connection-between-generative-adversarial-networks-inverse-reinforcement-learning-and-energy-based-models 	| https://arxiv.org/abs/1611.03852 	|
| August 18, 2017 	| shun-liao 	| OpenAI Baselines: ACKTR & A2C 	| https://openai.com/research/openai-baselines-acktr-a2c 	| https://arxiv.org/abs/1708.05144 	|
| May 28, 2022 	| stephanie-lin 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| September 8, 2021 	| stephanie-lin 	| TruthfulQA: Measuring how models mimic human falsehoods 	| https://openai.com/research/truthfulqa 	| https://arxiv.org/abs/2109.07958 	|
| May 28, 2020 	| mateusz-litwin 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| October 15, 2019 	| mateusz-litwin 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| December 4, 2017 	| christos-louizos 	| Learning sparse neural networks through L₀ regularization 	| https://openai.com/research/learning-sparse-neural-networks-through-l0-regularization 	| https://arxiv.org/abs/1712.01312 	|
| January 27, 2022 	| ryan-lowe 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| September 23, 2021 	| ryan-lowe 	| Summarizing books with human feedback 	| https://openai.com/research/summarizing-books 	|  	|
| September 4, 2020 	| ryan-lowe 	| Learning to summarize with human feedback 	| https://openai.com/research/learning-to-summarize-with-human-feedback 	| https://arxiv.org/abs/2009.01325 	|
| June 8, 2017 	| ryan-lowe 	| Learning to cooperate, compete, and communicate 	| https://openai.com/research/learning-to-cooperate-compete-and-communicate 	| https://arxiv.org/abs/1706.02275 	|
| March 16, 2017 	| ryan-lowe 	| Learning to communicate 	| https://openai.com/research/learning-to-communicate 	|  	|
| November 5, 2018 	| kendall-lowrey 	| Plan online, learn offline: Efficient learning and exploration via model-based control 	| https://openai.com/research/plan-online-learn-offline 	| https://arxiv.org/abs/1811.01848 	|
| February 14, 2019 	| david-luan 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| June 25, 2018 	| david-luan 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| May 28, 2020 	| benjamin-mann 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| September 17, 2019 	| todor-markov 	| Emergent tool use from multi-agent interaction 	| https://openai.com/research/emergent-tool-use 	| https://arxiv.org/abs/1909.07528 	|
| July 1, 2017 	| tambet-matiisen 	| Teacher–student curriculum learning 	| https://openai.com/research/teacher-student-curriculum-learning 	| https://arxiv.org/abs/1707.00183 	|
| March 3, 2022 	| katie-mayer 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| July 7, 2021 	| katie-mayer 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| July 7, 2021 	| sam-mccandlish 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 28, 2020 	| sam-mccandlish 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| January 23, 2020 	| sam-mccandlish 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| December 14, 2018 	| sam-mccandlish 	| How AI training scales 	| https://openai.com/research/how-ai-training-scales 	| https://arxiv.org/pdf/1812.06162.pdf 	|
| July 7, 2021 	| bob-mcgrew 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 15, 2019 	| bob-mcgrew 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| September 17, 2019 	| bob-mcgrew 	| Emergent tool use from multi-agent interaction 	| https://openai.com/research/emergent-tool-use 	| https://arxiv.org/abs/1909.07528 	|
| July 30, 2018 	| bob-mcgrew 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| bob-mcgrew 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| bob-mcgrew 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| October 19, 2017 	| bob-mcgrew 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 17, 2017 	| bob-mcgrew 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| July 5, 2017 	| bob-mcgrew 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| May 16, 2017 	| bob-mcgrew 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| July 28, 2022 	| christine-mcleavey-payne 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| April 30, 2020 	| christine-mcleavey-payne 	| Jukebox 	| https://openai.com/research/jukebox 	| https://arxiv.org/abs/2005.00341 	|
| March 15, 2018 	| dimitris-metaxas 	| Improving GANs using optimal transport 	| https://openai.com/research/improving-gans-using-optimal-transport 	| https://arxiv.org/abs/1803.05573 	|
| February 15, 2018 	| smitha-milli 	| Interpretable machine learning through teaching 	| https://openai.com/research/interpretable-machine-learning-through-teaching 	| https://arxiv.org/abs/1711.00694 	|
| November 2, 2017 	| smitha-milli 	| Interpretable and pedagogical examples 	| https://openai.com/research/interpretable-and-pedagogical-examples 	| https://arxiv.org/abs/1711.00694 	|
| December 16, 2022 	| pamela-mishkin 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| July 25, 2022 	| pamela-mishkin 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| March 3, 2022 	| pamela-mishkin 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| July 7, 2021 	| pamela-mishkin 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 5, 2021 	| pamela-mishkin 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| March 12, 2017 	| nikhil-mishra 	| Prediction and control with temporal segment models 	| https://openai.com/research/prediction-and-control-with-temporal-segment-models 	| https://arxiv.org/abs/1703.04070 	|
| July 7, 2021 	| vedant-misra 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 5, 2021 	| vedant-misra 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| May 25, 2016 	| takeru-miyato 	| Adversarial training methods for semi-supervised text classification 	| https://openai.com/research/adversarial-training-methods-for-semi-supervised-text-classification 	| https://arxiv.org/abs/1605.07725 	|
| September 17, 2019 	| igor-mordatch 	| Emergent tool use from multi-agent interaction 	| https://openai.com/research/emergent-tool-use 	| https://arxiv.org/abs/1909.07528 	|
| March 21, 2019 	| igor-mordatch 	| Implicit generation and generalization methods for energy-based models 	| https://openai.com/research/energy-based-models 	| http://arxiv.org/abs/1903.08689 	|
| March 4, 2019 	| igor-mordatch 	| Neural MMO: A massively multiagent game environment 	| https://openai.com/research/neural-mmo 	| http://arxiv.org/abs/1903.00784 	|
| November 7, 2018 	| igor-mordatch 	| Learning concepts with energy functions 	| https://openai.com/research/learning-concepts-with-energy-functions 	| https://arxiv.org/abs/1811.02486 	|
| November 5, 2018 	| igor-mordatch 	| Plan online, learn offline: Efficient learning and exploration via model-based control 	| https://openai.com/research/plan-online-learn-offline 	| https://arxiv.org/abs/1811.01848 	|
| March 20, 2018 	| igor-mordatch 	| Variance reduction for policy gradient with action-dependent factorized baselines 	| https://openai.com/research/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines 	| https://arxiv.org/abs/1803.07246 	|
| February 15, 2018 	| igor-mordatch 	| Interpretable machine learning through teaching 	| https://openai.com/research/interpretable-machine-learning-through-teaching 	| https://arxiv.org/abs/1711.00694 	|
| November 2, 2017 	| igor-mordatch 	| Interpretable and pedagogical examples 	| https://openai.com/research/interpretable-and-pedagogical-examples 	| https://arxiv.org/abs/1711.00694 	|
| October 11, 2017 	| igor-mordatch 	| Meta-learning for wrestling 	| https://openai.com/research/meta-learning-for-wrestling 	| https://arxiv.org/abs/1710.03641 	|
| October 11, 2017 	| igor-mordatch 	| Competitive self-play 	| https://openai.com/research/competitive-self-play 	| https://arxiv.org/abs/1710.03748 	|
| September 14, 2017 	| igor-mordatch 	| Learning to model other minds 	| https://openai.com/research/learning-to-model-other-minds 	| https://arxiv.org/abs/1709.04326 	|
| September 13, 2017 	| igor-mordatch 	| Learning with opponent-learning awareness 	| https://openai.com/research/learning-with-opponent-learning-awareness 	| https://arxiv.org/abs/1709.04326 	|
| June 8, 2017 	| igor-mordatch 	| Learning to cooperate, compete, and communicate 	| https://openai.com/research/learning-to-cooperate-compete-and-communicate 	| https://arxiv.org/abs/1706.02275 	|
| March 16, 2017 	| igor-mordatch 	| Learning to communicate 	| https://openai.com/research/learning-to-communicate 	|  	|
| March 15, 2017 	| igor-mordatch 	| Emergence of grounded compositional language in multi-agent populations 	| https://openai.com/research/emergence-of-grounded-compositional-language-in-multi-agent-populations 	| https://arxiv.org/abs/1703.04908 	|
| March 12, 2017 	| igor-mordatch 	| Prediction and control with temporal segment models 	| https://openai.com/research/prediction-and-control-with-temporal-segment-models 	| https://arxiv.org/abs/1703.04070 	|
| October 11, 2016 	| igor-mordatch 	| Transfer from simulation to real world through learning deep inverse dynamics model 	| https://openai.com/research/transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model 	| https://arxiv.org/abs/1610.03518 	|
| July 7, 2021 	| evan-morikawa 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| July 7, 2021 	| mira-murati 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| December 16, 2021 	| reiichiro-nakano 	| WebGPT: Improving the factual accuracy of language models through web browsing 	| https://openai.com/research/webgpt 	| https://arxiv.org/abs/2112.09332 	|
| December 5, 2019 	| preetum-nakkiran 	| Deep double descent 	| https://openai.com/research/deep-double-descent 	| https://arxiv.org/abs/1912.02292 	|
| February 4, 2019 	| preetum-nakkiran 	| Computational limitations in robust classification and win-win results 	| https://openai.com/research/computational-limitations-in-robust-classification-and-win-win-results 	| https://arxiv.org/abs/1902.01086 	|
| June 17, 2022 	| kamal-ndousse 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| January 24, 2022 	| arvind-neelakantan 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| May 28, 2020 	| arvind-neelakantan 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| December 16, 2022 	| alex-nichol 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| June 28, 2022 	| alex-nichol 	| DALL·E 2 pre-training mitigations 	| https://openai.com/research/dall-e-2-pre-training-mitigations 	|  	|
| April 13, 2022 	| alex-nichol 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| July 7, 2021 	| alex-nichol 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| June 22, 2018 	| alex-nichol 	| Retro Contest: Results 	| https://openai.com/research/retro-contest-results 	|  	|
| May 25, 2018 	| alex-nichol 	| Gym Retro 	| https://openai.com/research/gym-retro 	|  	|
| April 10, 2018 	| alex-nichol 	| Gotta Learn Fast: A new benchmark for generalization in RL 	| https://openai.com/research/gotta-learn-fast 	| https://arxiv.org/abs/1804.03720 	|
| April 5, 2018 	| alex-nichol 	| Retro Contest 	| https://openai.com/research/retro-contest 	| https://arxiv.org/abs/1804.03720 	|
| March 8, 2018 	| alex-nichol 	| On first-order meta-learning algorithms 	| https://openai.com/research/on-first-order-meta-learning-algorithms 	| https://arxiv.org/abs/1803.02999 	|
| March 7, 2018 	| alex-nichol 	| Reptile: A scalable meta-learning algorithm 	| https://openai.com/research/reptile 	| https://arxiv.org/abs/1803.02999 	|
| March 4, 2021 	| chris-olah 	| Multimodal neurons in artificial neural networks 	| https://openai.com/research/multimodal-neurons 	| https://distill.pub/2021/multimodal-neurons/ 	|
| April 14, 2020 	| chris-olah 	| OpenAI Microscope 	| https://openai.com/research/microscope 	|  	|
| March 6, 2019 	| chris-olah 	| Introducing Activation Atlases 	| https://openai.com/research/introducing-activation-atlases 	| https://distill.pub/2019/activation-atlas/ 	|
| July 1, 2017 	| avital-oliver 	| Teacher–student curriculum learning 	| https://openai.com/research/teacher-student-curriculum-learning 	| https://arxiv.org/abs/1707.00183 	|
| December 13, 2019 	| catherine-olsson 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| September 4, 2020 	| long-ouyang 	| Learning to summarize with human feedback 	| https://openai.com/research/learning-to-summarize-with-human-feedback 	| https://arxiv.org/abs/2009.01325 	|
| December 13, 2019 	| jakub-pachocki 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| July 30, 2018 	| jakub-pachocki 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| June 25, 2018 	| jakub-pachocki 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| October 11, 2017 	| jakub-pachocki 	| Competitive self-play 	| https://openai.com/research/competitive-self-play 	| https://arxiv.org/abs/1710.03748 	|
| February 20, 2018 	| michael-page 	| Preparing for malicious uses of AI 	| https://openai.com/research/preparing-for-malicious-uses-of-ai 	| https://arxiv.org/abs/1802.07228 	|
| July 7, 2021 	| alex-paino 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 15, 2019 	| alex-paino 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| February 24, 2017 	| nicolas-papernot 	| Attacking machine learning with adversarial examples 	| https://openai.com/research/attacking-machine-learning-with-adversarial-examples 	|  	|
| February 8, 2017 	| nicolas-papernot 	| Adversarial attacks on neural network policies 	| https://openai.com/research/adversarial-attacks-on-neural-network-policies 	| https://arxiv.org/abs/1702.02284 	|
| October 18, 2016 	| nicolas-papernot 	| Semi-supervised knowledge transfer for deep learning from private training data 	| https://openai.com/research/semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data 	| https://arxiv.org/abs/1610.05755 	|
| August 13, 2018 	| deepak-pathak 	| Large-scale study of curiosity-driven learning 	| https://openai.com/research/large-scale-study-of-curiosity-driven-learning 	| https://arxiv.org/abs/1808.04355 	|
| July 7, 2021 	| mikhail-pavlov 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 5, 2021 	| mikhail-pavlov 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| October 15, 2019 	| arthur-petron 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 30, 2018 	| arthur-petron 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| July 7, 2021 	| michael-petrov 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| March 4, 2021 	| michael-petrov 	| Multimodal neurons in artificial neural networks 	| https://openai.com/research/multimodal-neurons 	| https://distill.pub/2021/multimodal-neurons/ 	|
| April 14, 2020 	| michael-petrov 	| OpenAI Microscope 	| https://openai.com/research/microscope 	|  	|
| December 13, 2019 	| michael-petrov 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| michael-petrov 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| June 22, 2018 	| vicki-pfau 	| Retro Contest: Results 	| https://openai.com/research/retro-contest-results 	|  	|
| May 25, 2018 	| vicki-pfau 	| Gym Retro 	| https://openai.com/research/gym-retro 	|  	|
| April 10, 2018 	| vicki-pfau 	| Gotta Learn Fast: A new benchmark for generalization in RL 	| https://openai.com/research/gotta-learn-fast 	| https://arxiv.org/abs/1804.03720 	|
| April 5, 2018 	| vicki-pfau 	| Retro Contest 	| https://openai.com/research/retro-contest 	| https://arxiv.org/abs/1804.03720 	|
| October 19, 2017 	| lerrel-pinto 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 18, 2017 	| lerrel-pinto 	| Asymmetric actor critic for image-based robot learning 	| https://openai.com/research/asymmetric-actor-critic-for-image-based-robot-learning 	|  	|
| July 7, 2021 	| matthias-plappert 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 15, 2019 	| matthias-plappert 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 30, 2018 	| matthias-plappert 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| matthias-plappert 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| matthias-plappert 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| July 27, 2017 	| matthias-plappert 	| Better exploration with parameter noise 	| https://openai.com/research/better-exploration-with-parameter-noise 	| https://arxiv.org/abs/1706.01905 	|
| May 16, 2017 	| matthias-plappert 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| February 2, 2022 	| stanislas-polu 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| September 7, 2020 	| stanislas-polu 	| Generative language modeling for automated theorem proving 	| https://openai.com/research/generative-language-modeling-for-automated-theorem-proving 	| https://arxiv.org/abs/2009.03393 	|
| July 7, 2021 	| henrique-ponde 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| December 13, 2019 	| henrique-ponde 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| henrique-ponde 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| October 15, 2019 	| glenn-powell 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| September 17, 2019 	| glenn-powell 	| Emergent tool use from multi-agent interaction 	| https://openai.com/research/emergent-tool-use 	| https://arxiv.org/abs/1909.07528 	|
| July 30, 2018 	| glenn-powell 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| glenn-powell 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| glenn-powell 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| January 24, 2022 	| boris-power 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| alethea-power 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| November 2, 2016 	| eric-price 	| Extensions and limitations of the neural GPU 	| https://openai.com/research/extensions-and-limitations-of-the-neural-gpu 	| https://arxiv.org/abs/1611.00736 	|
| January 24, 2022 	| raul-puri 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| raul-puri 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 24, 2022 	| alec-radford 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| alec-radford 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 5, 2021 	| alec-radford 	| CLIP: Connecting text and images 	| https://openai.com/research/clip 	| https://arxiv.org/abs/2103.00020 	|
| June 17, 2020 	| alec-radford 	| Image GPT 	| https://openai.com/research/image-gpt 	| https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf 	|
| May 28, 2020 	| alec-radford 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| April 30, 2020 	| alec-radford 	| Jukebox 	| https://openai.com/research/jukebox 	| https://arxiv.org/abs/2005.00341 	|
| January 23, 2020 	| alec-radford 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| September 19, 2019 	| alec-radford 	| Fine-tuning GPT-2 from human preferences 	| https://openai.com/research/fine-tuning-gpt-2 	| https://arxiv.org/abs/1909.08593 	|
| February 14, 2019 	| alec-radford 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| June 25, 2018 	| alec-radford 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| June 11, 2018 	| alec-radford 	| Improving language understanding with unsupervised learning 	| https://openai.com/research/language-unsupervised 	| https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf 	|
| March 15, 2018 	| alec-radford 	| Improving GANs using optimal transport 	| https://openai.com/research/improving-gans-using-optimal-transport 	| https://arxiv.org/abs/1803.05573 	|
| December 6, 2017 	| alec-radford 	| Block-sparse GPU kernels 	| https://openai.com/research/block-sparse-gpu-kernels 	| https://cdn.openai.com/blocksparse/blocksparsepaper.pdf 	|
| August 18, 2017 	| alec-radford 	| OpenAI Baselines: ACKTR & A2C 	| https://openai.com/research/openai-baselines-acktr-a2c 	| https://arxiv.org/abs/1708.05144 	|
| July 20, 2017 	| alec-radford 	| Proximal Policy Optimization 	| https://openai.com/research/openai-baselines-ppo 	| https://arxiv.org/abs/1707.06347 	|
| April 6, 2017 	| alec-radford 	| Unsupervised sentiment neuron 	| https://openai.com/research/unsupervised-sentiment-neuron 	| https://arxiv.org/abs/1704.01444 	|
| December 13, 2019 	| jonathan-raiman 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| jonathan-raiman 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| November 5, 2018 	| aravind-rajeswaran 	| Plan online, learn offline: Efficient learning and exploration via model-based control 	| https://openai.com/research/plan-online-learn-offline 	| https://arxiv.org/abs/1811.01848 	|
| March 20, 2018 	| aravind-rajeswaran 	| Variance reduction for policy gradient with action-dependent factorized baselines 	| https://openai.com/research/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines 	| https://arxiv.org/abs/1803.07246 	|
| April 13, 2022 	| aditya-ramesh 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| January 5, 2021 	| aditya-ramesh 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| May 28, 2020 	| aditya-ramesh 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| July 7, 2021 	| alex-ray 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| November 21, 2019 	| alex-ray 	| Benchmarking safe exploration in deep reinforcement learning 	| https://openai.com/research/benchmarking-safe-exploration-in-deep-reinforcement-learning 	| https://cdn.openai.com/safexp-short.pdf 	|
| November 21, 2019 	| alex-ray 	| Safety Gym 	| https://openai.com/research/safety-gym 	| https://cdn.openai.com/safexp-short.pdf 	|
| July 30, 2018 	| alex-ray 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| alex-ray 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| alex-ray 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| October 19, 2017 	| alex-ray 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| July 5, 2017 	| alex-ray 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| June 28, 2017 	| alex-ray 	| Faster physics in Python 	| https://openai.com/research/faster-physics-in-python 	|  	|
| June 13, 2017 	| alex-ray 	| Learning from human preferences 	| https://openai.com/research/learning-from-human-preferences 	| https://arxiv.org/abs/1706.03741 	|
| May 16, 2017 	| alex-ray 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 1, 2017 	| alex-ray 	| Spam detection in the physical world 	| https://openai.com/research/spam-detection-in-the-physical-world 	|  	|
| May 16, 2017 	| erika-reinhardt 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| October 15, 2019 	| raphael-ribas 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 7, 2021 	| nick-ryder 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 28, 2020 	| nick-ryder 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| November 14, 2016 	| ruslan-salakhutdinov 	| On the quantitative analysis of decoder-based generative models 	| https://openai.com/research/on-the-quantitative-analysis-of-decoder-based-generative-models 	| https://arxiv.org/abs/1611.04273 	|
| December 13, 2019 	| tim-salimans 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| July 4, 2018 	| tim-salimans 	| Learning Montezuma’s Revenge from a single demonstration 	| https://openai.com/research/learning-montezumas-revenge-from-a-single-demonstration 	|  	|
| March 15, 2018 	| tim-salimans 	| Improving GANs using optimal transport 	| https://openai.com/research/improving-gans-using-optimal-transport 	| https://arxiv.org/abs/1803.05573 	|
| January 31, 2018 	| tim-salimans 	| Requests for Research 2.0 	| https://openai.com/research/requests-for-research-2 	|  	|
| March 24, 2017 	| tim-salimans 	| Evolution strategies as a scalable alternative to reinforcement learning 	| https://openai.com/research/evolution-strategies 	| https://arxiv.org/abs/1703.03864 	|
| January 19, 2017 	| tim-salimans 	| PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications 	| https://openai.com/research/pixelcnn-plus-plus 	| https://arxiv.org/abs/1701.05517 	|
| November 8, 2016 	| tim-salimans 	| Variational lossy autoencoder 	| https://openai.com/research/variational-lossy-autoencoder 	| https://arxiv.org/abs/1611.02731 	|
| June 16, 2016 	| tim-salimans 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| February 25, 2016 	| tim-salimans 	| Weight normalization: A simple reparameterization to accelerate training of deep neural networks 	| https://openai.com/research/weight-normalization 	| https://arxiv.org/abs/1602.07868 	|
| June 23, 2022 	| raul-sampedro 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| January 24, 2022 	| girish-sastry 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| girish-sastry 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 28, 2020 	| girish-sastry 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| May 16, 2018 	| girish-sastry 	| AI and compute 	| https://openai.com/research/ai-and-compute 	|  	|
| June 13, 2022 	| william-saunders 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| July 7, 2021 	| william-saunders 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| July 30, 2018 	| larissa-schiavo 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| June 25, 2018 	| larissa-schiavo 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| June 22, 2018 	| larissa-schiavo 	| Retro Contest: Results 	| https://openai.com/research/retro-contest-results 	|  	|
| May 25, 2018 	| larissa-schiavo 	| Gym Retro 	| https://openai.com/research/gym-retro 	|  	|
| April 5, 2018 	| larissa-schiavo 	| Retro Contest 	| https://openai.com/research/retro-contest 	| https://arxiv.org/abs/1804.03720 	|
| December 13, 2019 	| jeremy-schlatter 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| December 13, 2019 	| jonas-schneider 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| October 15, 2019 	| jonas-schneider 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 30, 2018 	| jonas-schneider 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| June 25, 2018 	| jonas-schneider 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| February 26, 2018 	| jonas-schneider 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| jonas-schneider 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| October 19, 2017 	| jonas-schneider 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 17, 2017 	| jonas-schneider 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| July 5, 2017 	| jonas-schneider 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| June 28, 2017 	| jonas-schneider 	| Faster physics in Python 	| https://openai.com/research/faster-physics-in-python 	|  	|
| May 16, 2017 	| jonas-schneider 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 1, 2017 	| jonas-schneider 	| Spam detection in the physical world 	| https://openai.com/research/spam-detection-in-the-physical-world 	|  	|
| March 21, 2017 	| jonas-schneider 	| One-shot imitation learning 	| https://openai.com/research/one-shot-imitation-learning 	| https://arxiv.org/abs/1703.07326 	|
| October 11, 2016 	| jonas-schneider 	| Transfer from simulation to real world through learning deep inverse dynamics model 	| https://openai.com/research/transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model 	| https://arxiv.org/abs/1610.03518 	|
| August 29, 2016 	| jonas-schneider 	| Infrastructure for deep learning 	| https://openai.com/research/infrastructure-for-deep-learning 	|  	|
| January 24, 2022 	| david-schnurr 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| April 14, 2020 	| ludwig-schubert 	| OpenAI Microscope 	| https://openai.com/research/microscope 	|  	|
| March 6, 2019 	| ludwig-schubert 	| Introducing Activation Atlases 	| https://openai.com/research/introducing-activation-atlases 	| https://distill.pub/2019/activation-atlas/ 	|
| October 19, 2022 	| john-schulman-2 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| July 28, 2022 	| john-schulman-2 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| December 16, 2021 	| john-schulman-2 	| WebGPT: Improving the factual accuracy of language models through web browsing 	| https://openai.com/research/webgpt 	| https://arxiv.org/abs/2112.09332 	|
| October 29, 2021 	| john-schulman-2 	| Solving math word problems 	| https://openai.com/research/solving-math-word-problems 	| http://arxiv.org/abs/2110.14168 	|
| December 3, 2019 	| john-schulman-2 	| Procgen Benchmark 	| https://openai.com/research/procgen-benchmark 	| https://arxiv.org/abs/1912.01588 	|
| June 25, 2018 	| john-schulman-2 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| June 22, 2018 	| john-schulman-2 	| Retro Contest: Results 	| https://openai.com/research/retro-contest-results 	|  	|
| May 25, 2018 	| john-schulman-2 	| Gym Retro 	| https://openai.com/research/gym-retro 	|  	|
| April 10, 2018 	| john-schulman-2 	| Gotta Learn Fast: A new benchmark for generalization in RL 	| https://openai.com/research/gotta-learn-fast 	| https://arxiv.org/abs/1804.03720 	|
| April 5, 2018 	| john-schulman-2 	| Retro Contest 	| https://openai.com/research/retro-contest 	| https://arxiv.org/abs/1804.03720 	|
| March 8, 2018 	| john-schulman-2 	| On first-order meta-learning algorithms 	| https://openai.com/research/on-first-order-meta-learning-algorithms 	| https://arxiv.org/abs/1803.02999 	|
| March 7, 2018 	| john-schulman-2 	| Reptile: A scalable meta-learning algorithm 	| https://openai.com/research/reptile 	| https://arxiv.org/abs/1803.02999 	|
| August 18, 2017 	| john-schulman-2 	| OpenAI Baselines: ACKTR & A2C 	| https://openai.com/research/openai-baselines-acktr-a2c 	| https://arxiv.org/abs/1708.05144 	|
| July 20, 2017 	| john-schulman-2 	| Proximal Policy Optimization 	| https://openai.com/research/openai-baselines-ppo 	| https://arxiv.org/abs/1707.06347 	|
| July 1, 2017 	| john-schulman-2 	| Teacher–student curriculum learning 	| https://openai.com/research/teacher-student-curriculum-learning 	| https://arxiv.org/abs/1707.00183 	|
| June 5, 2017 	| john-schulman-2 	| UCB exploration via Q-ensembles 	| https://openai.com/research/ucb-exploration-via-q-ensembles 	| https://arxiv.org/abs/1706.01502 	|
| May 24, 2017 	| john-schulman-2 	| OpenAI Baselines: DQN 	| https://openai.com/research/openai-baselines-dqn 	|  	|
| April 21, 2017 	| john-schulman-2 	| Equivalence between policy gradients and soft Q-learning 	| https://openai.com/research/equivalence-between-policy-gradients-and-soft-q-learning 	| https://arxiv.org/abs/1704.06440 	|
| November 15, 2016 	| john-schulman-2 	| #Exploration: A study of count-based exploration for deep reinforcement learning 	| https://openai.com/research/exploration 	| https://arxiv.org/abs/1611.04717 	|
| November 9, 2016 	| john-schulman-2 	| RL²: Fast reinforcement learning via slow reinforcement learning 	| https://openai.com/research/rl2 	| https://arxiv.org/abs/1611.02779 	|
| October 11, 2016 	| zain-shah 	| Transfer from simulation to real world through learning deep inverse dynamics model 	| https://openai.com/research/transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model 	| https://arxiv.org/abs/1610.03518 	|
| January 24, 2022 	| toki-sherbakov 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| January 24, 2022 	| pranav-shyam 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| May 28, 2020 	| pranav-shyam 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| December 13, 2019 	| szymon-sidor 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| July 30, 2018 	| szymon-sidor 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| June 25, 2018 	| szymon-sidor 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| October 11, 2017 	| szymon-sidor 	| Competitive self-play 	| https://openai.com/research/competitive-self-play 	| https://arxiv.org/abs/1710.03748 	|
| July 27, 2017 	| szymon-sidor 	| Better exploration with parameter noise 	| https://openai.com/research/better-exploration-with-parameter-noise 	| https://arxiv.org/abs/1706.01905 	|
| June 5, 2017 	| szymon-sidor 	| UCB exploration via Q-ensembles 	| https://openai.com/research/ucb-exploration-via-q-ensembles 	| https://arxiv.org/abs/1706.01502 	|
| May 24, 2017 	| szymon-sidor 	| OpenAI Baselines: DQN 	| https://openai.com/research/openai-baselines-dqn 	|  	|
| March 24, 2017 	| szymon-sidor 	| Evolution strategies as a scalable alternative to reinforcement learning 	| https://openai.com/research/evolution-strategies 	| https://arxiv.org/abs/1703.03864 	|
| January 25, 2021 	| eric-sigler 	| Scaling Kubernetes to 7,500 nodes 	| https://openai.com/research/scaling-kubernetes-to-7500-nodes 	|  	|
| May 28, 2020 	| eric-sigler 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| June 25, 2018 	| eric-sigler 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| June 10, 2021 	| irene-solaiman 	| Improving language model behavior by training on a curated dataset 	| https://openai.com/research/improving-language-model-behavior 	| https://cdn.openai.com/palms.pdf 	|
| June 2, 2018 	| dawn-song 	| GamePad: A learning environment for theorem proving 	| https://openai.com/research/gamepad 	| https://arxiv.org/abs/1806.00608 	|
| April 18, 2018 	| bradly-stadie 	| Evolved Policy Gradients 	| https://openai.com/research/evolved-policy-gradients 	| https://arxiv.org/abs/1802.04821 	|
| March 3, 2018 	| bradly-stadie 	| Some considerations on learning to explore via meta-reinforcement learning 	| https://openai.com/research/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning 	| https://arxiv.org/abs/1803.01118 	|
| May 16, 2017 	| bradly-stadie 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| March 21, 2017 	| bradly-stadie 	| One-shot imitation learning 	| https://openai.com/research/one-shot-imitation-learning 	| https://arxiv.org/abs/1703.07326 	|
| March 6, 2017 	| bradly-stadie 	| Third-person imitation learning 	| https://openai.com/research/third-person-imitation-learning 	| https://arxiv.org/abs/1703.01703 	|
| June 17, 2022 	| kenneth-o-stanley 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| March 17, 2023 	| nisan-stiennon 	| GPTs are GPTs: An early look at the labor market impact potential of large language models 	| https://openai.com/research/gpts-are-gpts 	| https://arxiv.org/abs/2303.10130 	|
| March 14, 2023 	| nisan-stiennon 	| GPT-4 	| https://openai.com/research/gpt-4 	| https://arxiv.org/abs/2303.08774 	|
| January 11, 2023 	| nisan-stiennon 	| Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk 	| https://openai.com/research/forecasting-misuse 	| https://arxiv.org/abs/2301.04246 	|
| December 16, 2022 	| nisan-stiennon 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| October 19, 2022 	| nisan-stiennon 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| September 21, 2022 	| nisan-stiennon 	| Introducing Whisper 	| https://openai.com/research/whisper 	| https://cdn.openai.com/papers/whisper.pdf 	|
| July 28, 2022 	| nisan-stiennon 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 25, 2022 	| nisan-stiennon 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| June 28, 2022 	| nisan-stiennon 	| DALL·E 2 pre-training mitigations 	| https://openai.com/research/dall-e-2-pre-training-mitigations 	|  	|
| June 23, 2022 	| nisan-stiennon 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| June 17, 2022 	| nisan-stiennon 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| June 13, 2022 	| nisan-stiennon 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| June 9, 2022 	| nisan-stiennon 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| May 28, 2022 	| nisan-stiennon 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| April 13, 2022 	| nisan-stiennon 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| April 13, 2022 	| nisan-stiennon 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| March 3, 2022 	| nisan-stiennon 	| A research agenda for assessing the economic impacts of code generation models 	| https://openai.com/research/economic-impacts 	| https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf 	|
| March 3, 2022 	| nisan-stiennon 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| February 2, 2022 	| nisan-stiennon 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| January 27, 2022 	| nisan-stiennon 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| November 15, 2016 	| adam-stooke 	| #Exploration: A study of count-based exploration for deep reinforcement learning 	| https://openai.com/research/exploration 	| https://arxiv.org/abs/1611.04717 	|
| August 13, 2018 	| amos-storkey 	| Large-scale study of curiosity-driven learning 	| https://openai.com/research/large-scale-study-of-curiosity-driven-learning 	| https://arxiv.org/abs/1808.04355 	|
| March 17, 2023 	| joseph-suarez 	| GPTs are GPTs: An early look at the labor market impact potential of large language models 	| https://openai.com/research/gpts-are-gpts 	| https://arxiv.org/abs/2303.10130 	|
| March 14, 2023 	| joseph-suarez 	| GPT-4 	| https://openai.com/research/gpt-4 	| https://arxiv.org/abs/2303.08774 	|
| January 11, 2023 	| joseph-suarez 	| Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk 	| https://openai.com/research/forecasting-misuse 	| https://arxiv.org/abs/2301.04246 	|
| December 16, 2022 	| joseph-suarez 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| October 19, 2022 	| joseph-suarez 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| September 21, 2022 	| joseph-suarez 	| Introducing Whisper 	| https://openai.com/research/whisper 	| https://cdn.openai.com/papers/whisper.pdf 	|
| July 28, 2022 	| joseph-suarez 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 25, 2022 	| joseph-suarez 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| June 28, 2022 	| joseph-suarez 	| DALL·E 2 pre-training mitigations 	| https://openai.com/research/dall-e-2-pre-training-mitigations 	|  	|
| June 23, 2022 	| joseph-suarez 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| June 17, 2022 	| joseph-suarez 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| June 13, 2022 	| joseph-suarez 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| June 9, 2022 	| joseph-suarez 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| May 28, 2022 	| joseph-suarez 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| April 13, 2022 	| joseph-suarez 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| April 13, 2022 	| joseph-suarez 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| March 3, 2022 	| joseph-suarez 	| A research agenda for assessing the economic impacts of code generation models 	| https://openai.com/research/economic-impacts 	| https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf 	|
| March 3, 2022 	| joseph-suarez 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| February 2, 2022 	| joseph-suarez 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| January 27, 2022 	| joseph-suarez 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| May 28, 2020 	| melanie-subbiah 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| January 24, 2022 	| felipe-petroski-such 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| felipe-petroski-such 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 3, 2019 	| yi-sun 	| Transfer of adversarial robustness between perturbation types 	| https://openai.com/research/transfer-of-adversarial-robustness-between-perturbation-types 	| https://arxiv.org/abs/1905.01034 	|
| February 2, 2022 	| ilya-sutskever 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| July 7, 2021 	| ilya-sutskever 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| January 5, 2021 	| ilya-sutskever 	| DALL·E: Creating images from text 	| https://openai.com/research/dall-e 	|  	|
| January 5, 2021 	| ilya-sutskever 	| CLIP: Connecting text and images 	| https://openai.com/research/clip 	| https://arxiv.org/abs/2103.00020 	|
| September 7, 2020 	| ilya-sutskever 	| Generative language modeling for automated theorem proving 	| https://openai.com/research/generative-language-modeling-for-automated-theorem-proving 	| https://arxiv.org/abs/2009.03393 	|
| June 17, 2020 	| ilya-sutskever 	| Image GPT 	| https://openai.com/research/image-gpt 	| https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf 	|
| May 28, 2020 	| ilya-sutskever 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| April 30, 2020 	| ilya-sutskever 	| Jukebox 	| https://openai.com/research/jukebox 	| https://arxiv.org/abs/2005.00341 	|
| December 13, 2019 	| ilya-sutskever 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| December 5, 2019 	| ilya-sutskever 	| Deep double descent 	| https://openai.com/research/deep-double-descent 	| https://arxiv.org/abs/1912.02292 	|
| February 14, 2019 	| ilya-sutskever 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| October 2, 2018 	| ilya-sutskever 	| FFJORD: Free-form continuous dynamics for scalable reversible generative models 	| https://openai.com/research/ffjord 	| https://arxiv.org/abs/1810.01367 	|
| July 30, 2018 	| ilya-sutskever 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| June 25, 2018 	| ilya-sutskever 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| June 2, 2018 	| ilya-sutskever 	| GamePad: A learning environment for theorem proving 	| https://openai.com/research/gamepad 	| https://arxiv.org/abs/1806.00608 	|
| May 16, 2018 	| ilya-sutskever 	| AI and compute 	| https://openai.com/research/ai-and-compute 	|  	|
| March 3, 2018 	| ilya-sutskever 	| Some considerations on learning to explore via meta-reinforcement learning 	| https://openai.com/research/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning 	| https://arxiv.org/abs/1803.01118 	|
| January 31, 2018 	| ilya-sutskever 	| Requests for Research 2.0 	| https://openai.com/research/requests-for-research-2 	|  	|
| October 11, 2017 	| ilya-sutskever 	| Meta-learning for wrestling 	| https://openai.com/research/meta-learning-for-wrestling 	| https://arxiv.org/abs/1710.03641 	|
| October 11, 2017 	| ilya-sutskever 	| Competitive self-play 	| https://openai.com/research/competitive-self-play 	| https://arxiv.org/abs/1710.03748 	|
| October 18, 2016 	| kunal-talwar 	| Semi-supervised knowledge transfer for deep learning from private training data 	| https://openai.com/research/semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data 	| https://arxiv.org/abs/1610.05755 	|
| June 8, 2017 	| aviv-tamar 	| Learning to cooperate, compete, and communicate 	| https://openai.com/research/learning-to-cooperate-compete-and-communicate 	| https://arxiv.org/abs/1706.02275 	|
| March 17, 2023 	| alex-tamkin 	| GPTs are GPTs: An early look at the labor market impact potential of large language models 	| https://openai.com/research/gpts-are-gpts 	| https://arxiv.org/abs/2303.10130 	|
| March 14, 2023 	| alex-tamkin 	| GPT-4 	| https://openai.com/research/gpt-4 	| https://arxiv.org/abs/2303.08774 	|
| January 11, 2023 	| alex-tamkin 	| Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk 	| https://openai.com/research/forecasting-misuse 	| https://arxiv.org/abs/2301.04246 	|
| December 16, 2022 	| alex-tamkin 	| Point-E: A system for generating 3D point clouds from complex prompts 	| https://openai.com/research/point-e 	| https://arxiv.org/abs/2212.08751 	|
| October 19, 2022 	| alex-tamkin 	| Scaling laws for reward model overoptimization 	| https://openai.com/research/scaling-laws-for-reward-model-overoptimization 	| https://arxiv.org/abs/2210.10760 	|
| September 21, 2022 	| alex-tamkin 	| Introducing Whisper 	| https://openai.com/research/whisper 	| https://cdn.openai.com/papers/whisper.pdf 	|
| July 28, 2022 	| alex-tamkin 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| July 25, 2022 	| alex-tamkin 	| A hazard analysis framework for code synthesis large language models 	| https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models 	| https://arxiv.org/abs/2207.14157 	|
| June 28, 2022 	| alex-tamkin 	| DALL·E 2 pre-training mitigations 	| https://openai.com/research/dall-e-2-pre-training-mitigations 	|  	|
| June 23, 2022 	| alex-tamkin 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| June 17, 2022 	| alex-tamkin 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| June 13, 2022 	| alex-tamkin 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| June 9, 2022 	| alex-tamkin 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| May 28, 2022 	| alex-tamkin 	| Teaching models to express their uncertainty in words 	| https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words 	| https://arxiv.org/abs/2205.14334 	|
| April 13, 2022 	| alex-tamkin 	| Hierarchical text-conditional image generation with CLIP latents 	| https://openai.com/research/hierarchical-text-conditional-image-generation-with-clip-latents 	| https://arxiv.org/abs/2204.06125 	|
| April 13, 2022 	| alex-tamkin 	| Measuring Goodhart’s law 	| https://openai.com/research/measuring-goodharts-law 	|  	|
| March 3, 2022 	| alex-tamkin 	| A research agenda for assessing the economic impacts of code generation models 	| https://openai.com/research/economic-impacts 	| https://cdn.openai.com/papers/Economic_Impacts_Research_Agenda.pdf 	|
| March 3, 2022 	| alex-tamkin 	| Lessons learned on language model safety and misuse 	| https://openai.com/research/language-model-safety-and-misuse 	|  	|
| February 2, 2022 	| alex-tamkin 	| Solving (some) formal math olympiad problems 	| https://openai.com/research/formal-math 	| https://arxiv.org/abs/2202.01344 	|
| January 27, 2022 	| alex-tamkin 	| Aligning language models to follow instructions 	| https://openai.com/research/instruction-following 	| https://arxiv.org/abs/2203.02155 	|
| June 23, 2022 	| jie-tang 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| July 7, 2021 	| jie-tang 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| December 13, 2019 	| jie-tang 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| jie-tang 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| November 15, 2016 	| haoran-tang 	| #Exploration: A study of count-based exploration for deep reinforcement learning 	| https://openai.com/research/exploration 	| https://arxiv.org/abs/1611.04717 	|
| January 24, 2022 	| madeleine-thompson 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 28, 2021 	| philippe-tillet 	| Introducing Triton: Open-source GPU programming for neural networks 	| https://openai.com/research/triton 	|  	|
| July 7, 2021 	| philippe-tillet 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| July 30, 2018 	| josh-tobin 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| josh-tobin 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| josh-tobin 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| October 19, 2017 	| josh-tobin 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 17, 2017 	| josh-tobin 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| July 5, 2017 	| josh-tobin 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| May 16, 2017 	| josh-tobin 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 1, 2017 	| josh-tobin 	| Spam detection in the physical world 	| https://openai.com/research/spam-detection-in-the-physical-world 	|  	|
| October 11, 2016 	| josh-tobin 	| Transfer from simulation to real world through learning deep inverse dynamics model 	| https://openai.com/research/transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model 	| https://arxiv.org/abs/1610.03518 	|
| November 5, 2018 	| emanuel-todorov 	| Plan online, learn offline: Efficient learning and exploration via model-based control 	| https://openai.com/research/plan-online-learn-offline 	| https://arxiv.org/abs/1811.01848 	|
| July 28, 2022 	| jerry-tworek 	| Efficient training of language models to fill in the middle 	| https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle 	| https://arxiv.org/abs/2207.14255 	|
| January 24, 2022 	| jerry-tworek 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| jerry-tworek 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 15, 2019 	| jerry-tworek 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| February 4, 2019 	| vinod-vaikuntanathan 	| Computational limitations in robust classification and win-win results 	| https://openai.com/research/computational-limitations-in-robust-classification-and-win-win-results 	| https://arxiv.org/abs/1902.01086 	|
| March 4, 2021 	| chelsea-voss 	| Multimodal neurons in artificial neural networks 	| https://openai.com/research/multimodal-neurons 	| https://distill.pub/2021/multimodal-neurons/ 	|
| September 4, 2020 	| chelsea-voss 	| Learning to summarize with human feedback 	| https://openai.com/research/learning-to-summarize-with-human-feedback 	| https://arxiv.org/abs/2009.01325 	|
| March 4, 2021 	| justin-jay-wang 	| Multimodal neurons in artificial neural networks 	| https://openai.com/research/multimodal-neurons 	| https://distill.pub/2021/multimodal-neurons/ 	|
| January 24, 2022 	| peter-welinder 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| peter-welinder 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 15, 2019 	| peter-welinder 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 30, 2018 	| peter-welinder 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| peter-welinder 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| peter-welinder 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| October 19, 2017 	| peter-welinder 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 18, 2017 	| peter-welinder 	| Asymmetric actor critic for image-based robot learning 	| https://openai.com/research/asymmetric-actor-critic-for-image-based-robot-learning 	|  	|
| October 17, 2017 	| peter-welinder 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| July 5, 2017 	| peter-welinder 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| June 28, 2017 	| peter-welinder 	| Faster physics in Python 	| https://openai.com/research/faster-physics-in-python 	|  	|
| May 16, 2017 	| peter-welinder 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| December 4, 2017 	| max-welling 	| Learning sparse neural networks through L₀ regularization 	| https://openai.com/research/learning-sparse-neural-networks-through-l0-regularization 	| https://arxiv.org/abs/1712.01312 	|
| June 9, 2022 	| lilian-weng 	| Techniques for training large neural networks 	| https://openai.com/research/techniques-for-training-large-neural-networks 	|  	|
| January 24, 2022 	| lilian-weng 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| October 15, 2019 	| lilian-weng 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 30, 2018 	| lilian-weng 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| September 14, 2017 	| shimon-whiteson 	| Learning to model other minds 	| https://openai.com/research/learning-to-model-other-minds 	| https://arxiv.org/abs/1709.04326 	|
| September 13, 2017 	| shimon-whiteson 	| Learning with opponent-learning awareness 	| https://openai.com/research/learning-with-opponent-learning-awareness 	| https://arxiv.org/abs/1709.04326 	|
| July 7, 2021 	| clemens-winter 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| May 28, 2020 	| clemens-winter 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| December 13, 2019 	| filip-wolski 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| filip-wolski 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| April 18, 2018 	| filip-wolski 	| Evolved Policy Gradients 	| https://openai.com/research/evolved-policy-gradients 	| https://arxiv.org/abs/1802.04821 	|
| July 20, 2017 	| filip-wolski 	| Proximal Policy Optimization 	| https://openai.com/research/openai-baselines-ppo 	| https://arxiv.org/abs/1707.06347 	|
| July 5, 2017 	| filip-wolski 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| May 16, 2017 	| filip-wolski 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| September 17, 2019 	| yi-wu 	| Emergent tool use from multi-agent interaction 	| https://openai.com/research/emergent-tool-use 	| https://arxiv.org/abs/1909.07528 	|
| June 8, 2017 	| yi-wu 	| Learning to cooperate, compete, and communicate 	| https://openai.com/research/learning-to-cooperate-compete-and-communicate 	| https://arxiv.org/abs/1706.02275 	|
| March 3, 2018 	| yuhuai-wu 	| Some considerations on learning to explore via meta-reinforcement learning 	| https://openai.com/research/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning 	| https://arxiv.org/abs/1803.01118 	|
| August 18, 2017 	| yuhuai-wu 	| OpenAI Baselines: ACKTR & A2C 	| https://openai.com/research/openai-baselines-acktr-a2c 	| https://arxiv.org/abs/1708.05144 	|
| November 14, 2016 	| yuhuai-wu 	| On the quantitative analysis of decoder-based generative models 	| https://openai.com/research/on-the-quantitative-analysis-of-decoder-based-generative-models 	| https://arxiv.org/abs/1611.04273 	|
| June 13, 2022 	| jeffrey-wu 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| September 23, 2021 	| jeffrey-wu 	| Summarizing books with human feedback 	| https://openai.com/research/summarizing-books 	|  	|
| September 4, 2020 	| jeffrey-wu 	| Learning to summarize with human feedback 	| https://openai.com/research/learning-to-summarize-with-human-feedback 	| https://arxiv.org/abs/2009.01325 	|
| May 28, 2020 	| jeffrey-wu 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| January 23, 2020 	| jeffrey-wu 	| Scaling laws for neural language models 	| https://openai.com/research/scaling-laws-for-neural-language-models 	| https://arxiv.org/abs/2001.08361 	|
| September 19, 2019 	| jeffrey-wu 	| Fine-tuning GPT-2 from human preferences 	| https://openai.com/research/fine-tuning-gpt-2 	| https://arxiv.org/abs/1909.08593 	|
| February 14, 2019 	| jeffrey-wu 	| Better language models and their implications 	| https://openai.com/research/better-language-models 	| https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf 	|
| March 20, 2018 	| cathy-wu 	| Variance reduction for policy gradient with action-dependent factorized baselines 	| https://openai.com/research/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines 	| https://arxiv.org/abs/1803.07246 	|
| January 24, 2022 	| tao-xu 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| December 5, 2019 	| tristan-yang 	| Deep double descent 	| https://openai.com/research/deep-double-descent 	| https://arxiv.org/abs/1912.02292 	|
| March 3, 2018 	| ge-yang 	| Some considerations on learning to explore via meta-reinforcement learning 	| https://openai.com/research/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning 	| https://arxiv.org/abs/1803.01118 	|
| June 13, 2022 	| catherine-yeh 	| AI-written critiques help humans notice flaws 	| https://openai.com/research/critiques 	| https://arxiv.org/abs/2206.05802 	|
| June 17, 2022 	| cathy-yeh 	| Evolution through large models 	| https://openai.com/research/evolution-through-large-models 	| https://arxiv.org/abs/2206.08896 	|
| July 30, 2018 	| diane-yoon 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| June 25, 2018 	| diane-yoon 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| January 24, 2022 	| qiming-yuan 	| Text and code embeddings by contrastive pre-training 	| https://openai.com/research/text-and-code-embeddings-by-contrastive-pre-training 	| https://arxiv.org/abs/2201.10005 	|
| July 7, 2021 	| qiming-yuan 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 15, 2019 	| qiming-yuan 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 7, 2021 	| wojciech-zaremba 	| Evaluating large language models trained on code 	| https://openai.com/research/evaluating-large-language-models-trained-on-code 	| https://arxiv.org/abs/2107.03374 	|
| October 15, 2019 	| wojciech-zaremba 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| July 30, 2018 	| wojciech-zaremba 	| Learning dexterity 	| https://openai.com/research/learning-dexterity 	| https://arxiv.org/abs/1808.00177 	|
| February 26, 2018 	| wojciech-zaremba 	| Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research 	| https://openai.com/research/multi-goal-reinforcement-learning 	| https://arxiv.org/abs/1802.09464 	|
| February 26, 2018 	| wojciech-zaremba 	| Ingredients for robotics research 	| https://openai.com/research/ingredients-for-robotics-research 	| https://arxiv.org/abs/1802.09464 	|
| October 19, 2017 	| wojciech-zaremba 	| Generalizing from simulation 	| https://openai.com/research/generalizing-from-simulation 	|  	|
| October 18, 2017 	| wojciech-zaremba 	| Asymmetric actor critic for image-based robot learning 	| https://openai.com/research/asymmetric-actor-critic-for-image-based-robot-learning 	|  	|
| October 18, 2017 	| wojciech-zaremba 	| Sim-to-real transfer of robotic control with dynamics randomization 	| https://openai.com/research/sim-to-real-transfer-of-robotic-control-with-dynamics-randomization 	| https://arxiv.org/abs/1710.06537 	|
| October 17, 2017 	| wojciech-zaremba 	| Domain randomization and generative models for robotic grasping 	| https://openai.com/research/domain-randomization-and-generative-models-for-robotic-grasping 	| https://arxiv.org/abs/1710.06425 	|
| July 5, 2017 	| wojciech-zaremba 	| Hindsight Experience Replay 	| https://openai.com/research/hindsight-experience-replay 	| https://arxiv.org/abs/1707.01495 	|
| June 28, 2017 	| wojciech-zaremba 	| Faster physics in Python 	| https://openai.com/research/faster-physics-in-python 	|  	|
| May 16, 2017 	| wojciech-zaremba 	| Robots that learn 	| https://openai.com/research/robots-that-learn 	|  	|
| April 1, 2017 	| wojciech-zaremba 	| Spam detection in the physical world 	| https://openai.com/research/spam-detection-in-the-physical-world 	|  	|
| March 21, 2017 	| wojciech-zaremba 	| One-shot imitation learning 	| https://openai.com/research/one-shot-imitation-learning 	| https://arxiv.org/abs/1703.07326 	|
| November 2, 2016 	| wojciech-zaremba 	| Extensions and limitations of the neural GPU 	| https://openai.com/research/extensions-and-limitations-of-the-neural-gpu 	| https://arxiv.org/abs/1611.00736 	|
| October 11, 2016 	| wojciech-zaremba 	| Transfer from simulation to real world through learning deep inverse dynamics model 	| https://openai.com/research/transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model 	| https://arxiv.org/abs/1610.03518 	|
| June 16, 2016 	| wojciech-zaremba 	| Generative models 	| https://openai.com/research/generative-models 	|  	|
| December 13, 2019 	| susan-zhang 	| Dota 2 with large scale deep reinforcement learning 	| https://openai.com/research/dota-2-with-large-scale-deep-reinforcement-learning 	| https://arxiv.org/abs/1912.06680 	|
| June 25, 2018 	| susan-zhang 	| OpenAI Five 	| https://openai.com/research/openai-five 	|  	|
| October 15, 2019 	| lei-zhang 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| March 15, 2018 	| han-zhang 	| Improving GANs using optimal transport 	| https://openai.com/research/improving-gans-using-optimal-transport 	| https://arxiv.org/abs/1803.05573 	|
| June 23, 2022 	| peter-zhokhov 	| Learning to play Minecraft with Video PreTraining 	| https://openai.com/research/vpt 	| https://arxiv.org/abs/2206.11795 	|
| September 4, 2020 	| daniel-ziegler 	| Learning to summarize with human feedback 	| https://openai.com/research/learning-to-summarize-with-human-feedback 	| https://arxiv.org/abs/2009.01325 	|
| May 28, 2020 	| daniel-ziegler 	| Language models are few-shot learners 	| https://openai.com/research/language-models-are-few-shot-learners 	| https://arxiv.org/abs/2005.14165 	|
| September 19, 2019 	| daniel-ziegler 	| Fine-tuning GPT-2 from human preferences 	| https://openai.com/research/fine-tuning-gpt-2 	| https://arxiv.org/abs/1909.08593 	|
| March 14, 2023 	| openai 	| GPT-4 	| https://openai.com/research/gpt-4 	| https://arxiv.org/abs/2303.08774 	|
| October 15, 2019 	| openai 	| Solving Rubik’s Cube with a robot hand 	| https://openai.com/research/solving-rubiks-cube 	| https://arxiv.org/abs/1910.07113 	|
| April 15, 2019 	| openai 	| OpenAI Five defeats Dota 2 world champions 	| https://openai.com/research/openai-five-defeats-dota-2-world-champions 	|  	|
| August 23, 2018 	| openai 	| The International 2018: Results 	| https://openai.com/research/the-international-2018-results 	|  	|
| August 6, 2018 	| openai 	| OpenAI Five Benchmark: Results 	| https://openai.com/research/openai-five-benchmark-results 	|  	|
| August 16, 2017 	| openai 	| More on Dota 2 	| https://openai.com/research/more-on-dota-2 	|  	|
| August 11, 2017 	| openai 	| Dota 2 	| https://openai.com/research/dota-2 	|  	|
| December 5, 2016 	| openai 	| Universe 	| https://openai.com/research/universe 	|  	|
